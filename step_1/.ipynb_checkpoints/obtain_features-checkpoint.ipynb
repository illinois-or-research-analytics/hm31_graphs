{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f303592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T17:37:55.356128568Z",
     "start_time": "2023-09-06T17:37:52.917065300Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98753it [00:00, 1033160.15it/s]\n",
      "2004it [00:00, 1118778.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#Load edges and nodes\n",
    "\n",
    "import numpy as np\n",
    "from load_data import *\n",
    "\n",
    "nodes_array, edge_array = assert_edges_are_within_first_cluster()\n",
    "\n",
    "node_lookup_dict = {}\n",
    "\n",
    "min_index = np.amin(nodes_array)\n",
    "max_index = np.amax(nodes_array)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for node in nodes_array:\n",
    "    node_lookup_dict[str(node)] = i\n",
    "    i+= 1\n",
    "    \n",
    "adj_matrix = np.load('adj_matrix.npy')    \n",
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeaad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T17:37:59.411951023Z",
     "start_time": "2023-09-06T17:37:55.339152442Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get dois from the exosome csv\n",
    "#Read the DOIS from the node id\n",
    "\n",
    "from load_data import *\n",
    "\n",
    "import csv\n",
    "\n",
    "csv_file_path = data_dir + 'exosome.csv'\n",
    "doi_lookup_dict = {}\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(csv_file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    dois = {}\n",
    "    for line_number, row in tqdm(enumerate(csv_reader), total = 14695476):  # 'total' is the total number of iterations\n",
    "        \n",
    "        if line_number <min_index or line_number > max_index:\n",
    "            continue\n",
    "            \n",
    "        id = str(row[0])\n",
    "        doi = row[2]\n",
    "        \n",
    "        if id in node_lookup_dict:\n",
    "            doi_lookup_dict[id] = doi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c810cf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.410994302Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save\n",
    "import json\n",
    "\n",
    "with open('first_cluster_dois.json', 'w') as json_file:\n",
    "    json.dump(doi_lookup_dict, json_file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f66d16",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.411400567Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get PMID by doi\n",
    "\n",
    "import requests\n",
    "import xmltodict\n",
    "\n",
    "doi_dict = {}\n",
    "\n",
    "#Get dois\n",
    "def fetch_pmid_from_doi(doi='10.1073/pnas.0510928103'):\n",
    "\n",
    "    pmid_dict = {}\n",
    "    request_str = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids='\n",
    "    request_str += str(doi)\n",
    "    \n",
    "    response = requests.get(request_str)\n",
    "    response_text = response.text\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        index = response_text.index('pmid=')\n",
    "\n",
    "\n",
    "        pmid_whole = response_text[index:].split(' ')[0]\n",
    "        pmid = int(pmid_whole[:-1].split('\\\"')[1])\n",
    "\n",
    "        return pmid\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "for key in tqdm(node_lookup_dict.keys()):\n",
    "    doi_dict[key] = fetch_pmid_from_doi(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e21bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.411517787Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#It turns out that initial node ids is their PMID\n",
    "\n",
    "\n",
    "for key, value in doi_dict.items():\n",
    "    assert key == value\n",
    "    \n",
    "\n",
    "with open('first_cluster_pmid.json', 'w') as json_file:\n",
    "    json.dump(doi_dict, json_file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce168bb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.411598379Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('first_cluster_pmid.json', 'r') as json_file:\n",
    "    doi_dict = json.load(json_file)\n",
    "    \n",
    "pmid_dict = doi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2af2d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.411734485Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save all xmls\n",
    "all_xmls = {}\n",
    "def save_all_xmls(pmid):\n",
    "    wait = 0.25\n",
    "    time.sleep(wait)\n",
    "    pmid_dict = {}\n",
    "    request_str  = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id='\n",
    "    request_str += str(pmid)\n",
    "    \n",
    "    response = requests.get(request_str)\n",
    "    response_text = response.text\n",
    "    \n",
    "    try:\n",
    "        xml_dict = xmltodict.parse(response_text)\n",
    "        \n",
    "    except:\n",
    "        time.sleep(2 * wait)\n",
    "        return fetch_metadata_from_pmid(pmid)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "# for id in tqdm(pmid_dict):\n",
    "#     all_xmls[id] = save_all_xmls(id)\n",
    "\n",
    "with open('first_cluster_xmls.json', 'w') as json_file:\n",
    "    json.dump(all_xmls, json_file, indent=4)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9485977d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.411817562Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load all XMLs\n",
    "import json\n",
    "\n",
    "with open('first_cluster_xmls.json', 'r') as json_file:\n",
    "    all_xmls = json.load(json_file)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d29decb9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.411898484Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2004/2004 [00:03<00:00, 510.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed parse 11 which are: ['11460150', '12335001', '5162804', '332795', '62782', '9877917', '4447963', '11441495', '12163935', '5377235', '410866']\n",
      "Occured keyword dict: {'keyword': 19, 'grant': 139, 'mesh': 1990, 'chemicallist': 1085, 'datecompleted': 1991, 'journal': 1993}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Mispelled\n",
    "import time\n",
    "import requests\n",
    "import xmltodict\n",
    "\n",
    "parsed_error = []\n",
    "dict_of_interest = {'keyword': 0, \"grant\": 0, \"mesh\": 0, \"chemicallist\":0, 'datecompleted': 0, 'journal': 0}\n",
    "journal_title_lookup_dict = {}\n",
    "journal_ISSN_lookup_dict = {}\n",
    "\n",
    "def fetch_metadata_from_pmid(response_text, idx, pmid): \n",
    "    global dict_of_interest\n",
    "    mesh_headings = []\n",
    "    grants = []\n",
    "    year = \"\"\n",
    "    journal_ISSN = \"\"\n",
    "    abstract = \"\"\n",
    "    chemical_list = []\n",
    "    meta_data = {}\n",
    "    journal_title = \"\"\n",
    "    pub_year = \"\"\n",
    "    \n",
    "    meta_data = {'mesh': mesh_headings, 'grants': grants, 'year': year, 'journal_ISSN': journal_ISSN,\n",
    "                 'journal_title': journal_title,'chemical' : chemical_list} \n",
    "    \n",
    "    try:\n",
    "        xml_dict = xmltodict.parse(response_text)\n",
    "        if len(str(xml_dict))< 100:\n",
    "            parsed_error.append(pmid)\n",
    "            return meta_data\n",
    "        \n",
    "        for key_of_interest in dict_of_interest.keys():\n",
    "            if key_of_interest in str(xml_dict).lower():\n",
    "\n",
    "                dict_of_interest[key_of_interest] += 1\n",
    "\n",
    "        #Date completed or revised?\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation'].keys())\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if 'DateCompleted' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "                if 'Year' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['DateCompleted']:\n",
    "                    year = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['DateCompleted']['Year']\n",
    "\n",
    "            else:\n",
    "    #             print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation'])\n",
    "                  pass\n",
    "    \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        \n",
    "        try:\n",
    "            if 'JournalIssue' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                if 'PubDate' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']:\n",
    "                    if 'Year' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']:\n",
    "                        pub_year = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "                        \n",
    "            if 'ISSN' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                journal_ISSN = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['ISSN']['#text'] \n",
    "            \n",
    "            if 'Title' in  xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                journal_title = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['Title']\n",
    "                \n",
    "            if len(journal_ISSN) > 0 and len(journal_title)>0:\n",
    "                journal_ISSN_lookup_dict[journal_ISSN] = journal_title\n",
    "                journal_title_lookup_dict[journal_title] = journal_ISSN\n",
    "                \n",
    "            elif len(journal_ISSN) == 0:\n",
    "                if journal_title in journal_title_lookup_dict:\n",
    "                    journal_ISSN = journal_title_lookup_dict[journal_title]\n",
    "                    \n",
    "            elif len(journal_title) == 0:\n",
    "                if journal_ISSN in journal_ISSN_lookup_dict:\n",
    "                    journal_title = journal_ISSN_lookup_dict[journal_ISSN]                \n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e, pmid)\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']))\n",
    "        \n",
    "        \n",
    "        #Grant\n",
    "        #Very few grants don't have grant id's\n",
    "        #Grant institute could also be relevant\n",
    "        #TODO: collect missing grant id\n",
    "\n",
    "        try:\n",
    "            if 'GrantList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']:\n",
    "                if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']) == list:\n",
    "                    for grant in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']:\n",
    "                        if 'GrantID' in grant:\n",
    "                            grants.append((grant['GrantID']))\n",
    "                            \n",
    "                else:\n",
    "                     grants.append(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']['GrantID'])\n",
    "                     pass   \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant'][0])\n",
    "        \n",
    "        #MeSH heading  \n",
    "        \n",
    "        try:\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']))\n",
    "        \n",
    "            #DescriptorName? Is a term\n",
    "            if 'MeshHeadingList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "                if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']) == list:\n",
    "                    for mesh in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']:\n",
    "    #                 print(mesh['DescriptorName'])\n",
    "    #                 print(mesh['DescriptorName']['@UI'])\n",
    "    #                 print(mesh['DescriptorName']['#text'])\n",
    "    \n",
    "                        if '@Type' in mesh['DescriptorName'].keys() and mesh['DescriptorName']['@Type'] == 'Geographic':\n",
    "                            continue\n",
    "                \n",
    "                \n",
    "                        mesh_headings.append((mesh['DescriptorName']['@UI'], mesh['DescriptorName']['#text']))\n",
    "            \n",
    "            else:\n",
    "                mesh_headings.append(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']['DescriptorName']['@UI'],\n",
    "                                    xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']['DescriptorName']['#text'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        try:\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']))\n",
    "        \n",
    "            #DescriptorName? Is a term\n",
    "            if 'ChemicalList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "#                print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']\n",
    "# #                      ['ChemicalList']['Chemical']['NameOfSubstance'])\n",
    "               #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']))\n",
    "               if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']) == list:\n",
    "                   for substance in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']:\n",
    "                            chemical_list.append(substance['NameOfSubstance']['@UI'])\n",
    "                        \n",
    "               else:\n",
    "                   chemical_list.append(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']['NameOfSubstance']['@UI'])\n",
    "                #print(chemical_list)\n",
    "                \n",
    "               \n",
    "              \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "        #References and history\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['PubmedData'].keys())\n",
    "\n",
    "\n",
    "    except:\n",
    "        parsed_error.append(idx)\n",
    "        pass\n",
    "\n",
    "\n",
    "    if len(year) == 0:\n",
    "        year = pub_year\n",
    "        \n",
    "    meta_data = {'mesh': mesh_headings, 'grants': grants, 'year': year, 'journal_ISSN': journal_ISSN,\n",
    "                 'journal_title': journal_title,'chemical' : chemical_list} \n",
    "        \n",
    "    return meta_data\n",
    "\n",
    "metadata_dict = {}\n",
    "\n",
    "# [166, 719, 1672, 1918] odd\n",
    "\n",
    "idx = 0\n",
    "for key, value in tqdm(all_xmls.items()):\n",
    "    metadata_dict[key] = fetch_metadata_from_pmid(value, idx, key)\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "\n",
    "print(f'failed parse {len(parsed_error)} which are: {parsed_error}')\n",
    "print(f'Occured keyword dict: {dict_of_interest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6803455",
   "metadata": {},
   "source": [
    "Turns out that out of 2004 papers of CEN cluster1, 11 of those don't have any metadata. That leaves us with 1993 samples. The above dictionary tells us the frequence of keyword occurence. The bellow one tells us the frequency of actual recorded value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07c2472e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T17:37:59.415602775Z",
     "start_time": "2023-09-06T17:37:59.412534170Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recorded features  {'mesh': 1982, 'grants': 136, 'year': 1993, 'journal_title': 1993, 'journal_ISSN': 1960, 'chemical': 1085}\n",
      "avg mesh length: 9.433400605449041 std: 4.450688949940417\n",
      "avg chemical length: 3.3788018433179725 std: 2.542416652594202\n"
     ]
    }
   ],
   "source": [
    "#### Count incomplete data\n",
    "\n",
    "features = {'mesh': 0, 'grants': 0, 'year': 0, 'journal_title': 0, \"journal_ISSN\":0, 'chemical' : 0}\n",
    "#Mesh terms are sometimes unrelated\n",
    "\n",
    "mesh_length = []\n",
    "chemical_length = []\n",
    "\n",
    "for feature in features.keys():\n",
    "    for _, meta in metadata_dict.items():\n",
    "        if len(meta[feature]) > 0:\n",
    "            features[feature] += 1\n",
    "            if feature == 'mesh':\n",
    "                mesh_length.append(len(meta[feature]))\n",
    "            \n",
    "            elif feature == 'chemical':\n",
    "                chemical_length.append(len(meta[feature]))\n",
    "                \n",
    "            \n",
    "print(\"recorded features \", features)\n",
    "print(f\"avg mesh length: {np.mean(mesh_length)} std: {np.std(mesh_length)}\" )\n",
    "print(f\"avg chemical length: {np.mean(chemical_length)} std: {np.std(chemical_length)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0c402",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.412631263Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('first_cluster_metadata.json', 'w') as json_file:\n",
    "    json.dump(metadata_dict, json_file, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b161bb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.412692739Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the metadata file\n",
    "with open('first_cluster_metadata.json', 'r') as json_file:\n",
    "    metadata_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9512e5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.450908544Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_dict_similarity(dict_1, dict_2, mode ): \n",
    "    similarity = 0\n",
    "    modes = ['number_of_common_terms', 'jaccard']\n",
    "    \n",
    "    assert mode in modes\n",
    "    \n",
    "    for key in dict_1:\n",
    "            if key in dict_2:\n",
    "                similarity += 1\n",
    "                \n",
    "    \n",
    "    if mode == 'number_of_common_terms':\n",
    "        pass\n",
    "       \n",
    "    elif mode == 'jaccard':\n",
    "        similarity = similarity / (len(dict_1) + len(dict_2))\n",
    "        \n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fb9ea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(int(2==2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec54f8d",
   "metadata": {},
   "source": [
    "<h1>Checking for same grants</h1> <p> Let's simply record number of same grants. It turns out no two papers have same grant</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08a7e782",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.451318586Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculate similarity between two pmid's\n",
    "#Lets call it metric for now. This is unnormalized\n",
    "def calculate_similarity(pmid1, pmid2, feature):\n",
    "    \n",
    "    metric = 0\n",
    "    pmid1 = str(pmid1)\n",
    "    pmid2 = str(pmid2)\n",
    "    \n",
    "    assert feature in ['year', 'mesh', 'chemical', 'co-citation', 'bib-coupling', 'grants', 'journal']\n",
    "    if feature == 'year':             \n",
    "                year1 = metadata_dict[pmid1][feature]\n",
    "                year2 = metadata_dict[pmid2][feature]\n",
    "                \n",
    "                if len(year1) > 0 and len(year2)>0:\n",
    "                    metric = np.abs(int(year1)-int(year2))\n",
    "                \n",
    "                else:\n",
    "                    metric = -1\n",
    "                    \n",
    "    if feature == 'journal':\n",
    "        issn1 = metadata_dict[pmid1]['journal_ISSN']\n",
    "        issn2 = metadata_dict[pmid2]['journal_ISSN']\n",
    "        \n",
    "        title1 = metadata_dict[pmid1]['journal_title']\n",
    "        title2 = metadata_dict[pmid2]['journal_title']\n",
    "        \n",
    "        \n",
    "        if len(issn1)>0 and len(issn2)>0:\n",
    "            metric = int(issn1==issn2)\n",
    "        \n",
    "        \n",
    "  \n",
    "        elif len(title1)>0 and len(title2)>0:\n",
    "            metric = int(title1==title2)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            metric = -1\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    #They are both treated the same way ...\n",
    "    elif feature == 'mesh' or feature == 'chemical':\n",
    "        common_terms = 0\n",
    "        terms_1 = {}\n",
    "        terms_2 = {}\n",
    "        \n",
    "        for term in metadata_dict[pmid1][feature]:\n",
    "            terms_1[term[0]] = 1\n",
    "    \n",
    "        for term in metadata_dict[pmid2][feature]:\n",
    "            terms_2[term[0]] = 1\n",
    "        \n",
    "        if len(terms_1) > 0 and len(terms_2)>0:\n",
    "            metric = calculate_dict_similarity(terms_1, terms_2, 'jaccard')\n",
    "                    \n",
    "        else:\n",
    "            metric = -1\n",
    "            \n",
    "    elif feature == 'bib-coupling':\n",
    "        node_1 = node_lookup_dict[pmid1]\n",
    "        node_2 = node_lookup_dict[pmid2]\n",
    "        \n",
    "        common = np.dot(adj_matrix[node_1,:], adj_matrix[node_2, :])\n",
    "        denom = np.sum(adj_matrix[node_1, :]) + np.sum(adj_matrix[node_2, :]) - common\n",
    "        \n",
    "        if denom > 0 :\n",
    "            metric = common / denom\n",
    "        \n",
    "    \n",
    "    elif feature == 'co-citation':\n",
    "        node_1 = node_lookup_dict[pmid1]\n",
    "        node_2 = node_lookup_dict[pmid2]\n",
    "        \n",
    "        common = np.dot(adj_matrix[: , node_1], adj_matrix[: , node_2])\n",
    "        denom = np.sum(adj_matrix[:, node_1]) + np.sum(adj_matrix[:, node_2]) - common\n",
    "        \n",
    "        if denom > 0 :\n",
    "            metric = common / denom\n",
    "            \n",
    "    \n",
    "    elif feature == 'grants':\n",
    "        grants_1 = metadata_dict[pmid1]['grants']\n",
    "        grants_2 = metadata_dict[pmid2]['grants']\n",
    "       \n",
    "        \n",
    "        for first in grants_1:\n",
    "            for second in grants_2:\n",
    "                if first == second:\n",
    "                    metric +=1\n",
    "                    \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c8c7268",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.451391253Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_three_hop_similarity():\n",
    "    #Is currently inefficient as relies on matrix multiplication\n",
    "    # We use undirected edges\n",
    "    aggregated_three_hop_similarity = np.zeros_like(adj_matrix)\n",
    "    undirected_adj_matrix = adj_matrix + adj_matrix.transpose()\n",
    "    second_hop_distance = np.matmul(undirected_adj_matrix, undirected_adj_matrix)\n",
    "    \n",
    "    three_hop_distance = np.matmul(second_hop_distance, undirected_adj_matrix)\n",
    "    \n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        for j in range(i):\n",
    "            similarity = 0\n",
    "            if undirected_adj_matrix[i][j] == 1:\n",
    "                similarity = 1\n",
    "            \n",
    "            elif second_hop_distance[i][j] > 0:\n",
    "                similarity = 2/3\n",
    "            \n",
    "            elif three_hop_distance[i][j] > 0:\n",
    "                similarity = 1/3\n",
    "            \n",
    "            aggregated_three_hop_similarity[i][j] = similarity\n",
    "    \n",
    "    aggregated_three_hop_similarity = aggregated_three_hop_similarity + aggregated_three_hop_similarity.transpose()\n",
    "    return aggregated_three_hop_similarity\n",
    "\n",
    "aggregated_three_hop_similarity = calculate_three_hop_similarity()\n",
    "np.save('aggregated_three_hop_similarity.npy', aggregated_three_hop_similarity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3748c102",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.451548779Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh terms with one entires 0\n",
      "Mesh terms with two entires 18697\n",
      "Example of a mesh: ('D015046', 'Zoology')\n"
     ]
    }
   ],
   "source": [
    "#all mesh terms are tuples, so we use UI's as they are standard\n",
    "\n",
    "ones = 0\n",
    "twos = 0\n",
    "\n",
    "for key, value in metadata_dict.items():\n",
    "    if len(value['mesh']) >0:\n",
    "        for mesh in value['mesh']:\n",
    "            if len(mesh) == 1:\n",
    "                ones += 1\n",
    "            \n",
    "            elif len(mesh) == 2:\n",
    "                twos += 1\n",
    "            \n",
    "            else:\n",
    "                raise 'error'\n",
    "            \n",
    "print(f'Mesh terms with one entires {ones}')\n",
    "print(f'Mesh terms with two entires {twos}')\n",
    "print(f'Example of a mesh: {mesh}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bae28d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T17:37:59.470382501Z",
     "start_time": "2023-09-06T17:37:59.451614914Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2004/2004 [00:04<00:00, 458.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def report_stats(vals, title):\n",
    "    filtered_val = []\n",
    "    for val in vals:\n",
    "        if val != -1:\n",
    "            filtered_val.append(val)\n",
    "            \n",
    "    median = np.median(filtered_val)\n",
    "    mean = np.mean(filtered_val)\n",
    "    min = np.amin(filtered_val)\n",
    "    max = np.amax(filtered_val)\n",
    "    total = np.sum(filtered_val)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "#     fig.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(vals)\n",
    "    \n",
    "    labels = [title]\n",
    "\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    ax.set_title(f'{title}  boxplot diagram')\n",
    "#     ax.set_xlabel(f'{title}')\n",
    "    ax.set_ylabel('Values')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(f'median {median} mean {mean} min {min} max {max} total {total}')\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_similarity_matrix(feature):\n",
    "    similarity_matrix = np.zeros((len(nodes_array), len(nodes_array))) \n",
    "    \n",
    "    for i in tqdm(range(similarity_matrix.shape[0])):\n",
    "        for j in range(i):\n",
    "            pmid1 = str(nodes_array[i])\n",
    "            pmid2 = str(nodes_array[j])            \n",
    "            similarity_matrix[i][j] = calculate_similarity(pmid1, pmid2, feature)\n",
    "        \n",
    "    similarity_matrix = similarity_matrix + similarity_matrix.transpose()\n",
    "    \n",
    "    #Year is the only feature that is initially distance and needs to be similarity\n",
    "    if feature == 'year':\n",
    "        similarity_matrix = np.ones_like(similarity_matrix) - similarity_matrix/np.amax(similarity_matrix)\n",
    "        similarity_matrix = np.where(similarity_matrix > 1, -1, similarity_matrix)\n",
    "        \n",
    "    return similarity_matrix\n",
    "\n",
    "# year_similarity_matrix = calculate_similarity_matrix('year')\n",
    "# np.save('year_similarity_matrix.npy', year_similarity_matrix)\n",
    "\n",
    "# mesh_similarity_matrix = calculate_similarity_matrix('mesh')\n",
    "# np.save('mesh_similarity_matrix.npy', mesh_similarity_matrix)\n",
    "\n",
    "\n",
    "# bib_coupling_similarity_matrix = calculate_similarity_matrix('bib-coupling')\n",
    "# np.save('bib_coupling_similarity_matrix.npy', bib_coupling_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# cocitation_similarity_matrix = calculate_similarity_matrix('co-citation')\n",
    "# np.save('cocitation_similarity_matrix.npy', cocitation_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# chemical_similarity_matrix = calculate_similarity_matrix('chemical')\n",
    "# np.save('chemical_similarity_matrix.npy', chemical_similarity_matrix)\n",
    "\n",
    "\n",
    "# grants_similarity_matrix = calculate_similarity_matrix('grants')\n",
    "# np.save('grants_similarity_matrix.npy', grants_similarity_matrix)\n",
    "\n",
    "\n",
    "journal_similarity_matrix = calculate_similarity_matrix('journal')\n",
    "np.save('journal_similarity_matrix.npy', grants_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad8626da",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.451692620Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cocitation_matrix = np.load('cocitation_matrix.npy')\n",
    "bib_coupling_matrix = np.load('bib_coupling_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cad3cec",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.451767591Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median 0.0 mean 0.0008957052494977401 min 0.0 max 1.0 total 1778.0\n"
     ]
    }
   ],
   "source": [
    "#Assuming a square matrix, report statistics\n",
    "def report_matrix_stats(matrix, title):\n",
    "    n = matrix.shape[0]\n",
    "    all_values = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            #Remove negatives as they are flags\n",
    "            if matrix[i][j] > -1:\n",
    "                all_values.append(matrix[i][j])\n",
    "    \n",
    "    report_stats(all_values, title)\n",
    "    \n",
    "    \n",
    "# report_matrix_stats(year_similarity_matrix, 'year')\n",
    "\n",
    "# report_matrix_stats(mesh_similarity_matrix, 'jaccard mesh similarity')\n",
    "\n",
    "# report_matrix_stats(chemical_similarity_matrix, 'jaccard chemical similarity')\n",
    "\n",
    "# report_matrix_stats(cocitation_similarity_matrix, 'jaccard co-citation similarity')\n",
    "# report_matrix_stats(cocitation_matrix, 'raw co-citation similarity')\n",
    "\n",
    "# report_matrix_stats(bib_coupling_similarity_matrix, 'jaccard bib-coupling similarity')\n",
    "# report_matrix_stats(bib_coupling_matrix, 'raw bib-coupling similarity')\n",
    "\n",
    "\n",
    "# report_matrix_stats(aggregated_three_hop_similarity, 'aggregated three hop similarity')\n",
    "# report_matrix_stats(grants_similarity_matrix, 'grants similarity')\n",
    "\n",
    "report_matrix_stats(journal_similarity_matrix, 'journal similarity')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9a7ed",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.451940456Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.mean(grants_similarity_matrix))\n",
    "\n",
    "for key, value in metadata_dict.items():\n",
    "    if len(value['grants']) > 0:\n",
    "        print(value['grants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea75bd3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452013534Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Verify that those entries without grant actually do not have grants\n",
    "sample_list_of_pmid_without_grant = []\n",
    "without_grants_doi = []\n",
    "i = 0\n",
    "with_grants = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if 'grant' in str(value).lower():\n",
    "        with_grants.append(key)\n",
    "    \n",
    "    else:\n",
    "        sample_list_of_pmid_without_grant.append(key)\n",
    "        without_grants_doi.append(doi_lookup_dict[key])\n",
    "        \n",
    "    i+= 1\n",
    "    \n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "print('Statistics for first 10 papers:\\n')\n",
    "print(f'DOIs without grant: {without_grants_doi[0:10]}')\n",
    "print(f'PMIDs without grant: {sample_list_of_pmid_without_grant[0:10]}\\n')\n",
    "\n",
    "print(f'PMID of papers with grants {with_grants}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd746ba",
   "metadata": {},
   "source": [
    "<h1>Example</h1>\n",
    "An example of bib-couple and co-citation calculation for two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784fe88",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452149349Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_node_id = 0\n",
    "first_node_pmid = nodes_array[first_node_id]\n",
    "\n",
    "second_node_id = 50\n",
    "second_node_pmid = nodes_array[second_node_id]\n",
    "\n",
    "first_reference = {}\n",
    "first_cited = {}\n",
    "\n",
    "second_reference = {}\n",
    "second_cited = {}\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if start_node == first_node_pmid:\n",
    "        first_reference[end_node] = 1\n",
    "    if end_node == first_node_pmid:\n",
    "        first_cited[start_node] = 1     \n",
    "        \n",
    "    \n",
    "    if start_node == second_node_pmid:\n",
    "        second_reference[end_node] = 1\n",
    "    if end_node == second_node_pmid:\n",
    "        second_cited[start_node] = 1       \n",
    "\n",
    "print(f'recorded bib-coupling: {bib_coupling_matrix[first_node_id][second_node_id]}')\n",
    "print(f'crossed checked bib-coupling: {calculate_dict_similarity(first_reference, second_reference, mode = \"number_of_common_terms\")}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'recorded co-citation: {cocitation_matrix[first_node_id][second_node_id]}')\n",
    "print(f'crossed checked co-citation: {calculate_dict_similarity(first_cited, second_cited, mode = \"number_of_common_terms\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d49b37",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "<h1>Missing ISSN</h1>\n",
    "Finding xml's without ISSN. Any journal info?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea48bb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452232005Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_issn = []\n",
    "for key, value in all_xmls.items():\n",
    "    if not 'issn' in str(value).lower():\n",
    "        missing_issn.append(key)\n",
    "        \n",
    "print(f'missing ISSN pmids {missing_issn}')\n",
    "print(len(missing_issn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab282043",
   "metadata": {},
   "source": [
    "<h1>Examples of co-citation similarity of 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4b7d8",
   "metadata": {},
   "source": [
    "Interestingly, for all those of paris with jaccard_cocitation = 1, they have been only cited once! As we see below, citation counts of all of those pairs is 1. This could falsly inflate co_citation similarity. This will likely be the case with new publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff64255",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452316223Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year_similarity_matrix = np.load('year_similarity_matrix.npy')\n",
    "year_similarity_array = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "for i in range(cocitation_similarity_matrix.shape[0]):\n",
    "    for j in range(i):\n",
    "        if cocitation_similarity_matrix[i][j] > 0.99:\n",
    "            row_indices.append(i)\n",
    "            col_indices.append(j)\n",
    "\n",
    "for idx in range(len(row_indices)):\n",
    "    row = row_indices[idx]\n",
    "    col = col_indices[idx]\n",
    "    year_similarity_array.append(year_similarity_matrix[row][col])\n",
    "    print(f'citation count of both papers: {np.sum(adj_matrix[:,row])} cocitation Jaccard similarity: {cocitation_similarity_matrix[row][col]} Pmid1:{nodes_array[row]} Pmid1:{nodes_array[col]}')\n",
    "\n",
    "\n",
    "print(f'Average year similarity {np.average(year_similarity_array)} total cases {len(row_indices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b73db1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452376938Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bib_coupling_similarity_matrix = np.load('bib_coupling_similarity_matrix.npy')\n",
    "cocitation_similarity_matrix = np.load('cocitation_similarity_matrix.npy')\n",
    "\n",
    "#For co_citation of 1\n",
    "first_paper_id = np.unravel_index(cocitation_similarity_matrix.argmax(), cocitation_similarity_matrix.shape)[0]\n",
    "second_paper_id = np.unravel_index(cocitation_similarity_matrix.argmax(), cocitation_similarity_matrix.shape)[1]\n",
    "\n",
    "first_node_pmid =  nodes_array[first_paper_id]\n",
    "second_node_pmid =  nodes_array[second_paper_id]\n",
    "\n",
    "first_cited = []\n",
    "second_cited = []\n",
    "\n",
    "\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if end_node == first_node_pmid:\n",
    "        first_cited.append(start_node)   \n",
    "        \n",
    "    if end_node == second_node_pmid:\n",
    "        second_cited.append(start_node)\n",
    "        \n",
    "\n",
    "print(f'first paper pmid: {first_node_pmid} second paper pmid: {second_node_pmid}. Citing list:\\n')\n",
    "print(first_cited)\n",
    "\n",
    "#Assert two arrays are equal\n",
    "assert np.sum(np.sort(first_cited) - np.sort(second_cited)) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f863b",
   "metadata": {},
   "source": [
    "<h1>Examples of bib-coupling similarity of 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c1dcd",
   "metadata": {},
   "source": [
    "Here the situation is differen. As we expect, reference list is typically larger than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc92ca",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452436540Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row_indices, col_indices = np.where(bib_coupling_similarity_matrix == 1)\n",
    "year_similarity_array = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "\n",
    "for i in range(bib_coupling_similarity_matrix.shape[0]):\n",
    "    for j in range(i):\n",
    "        if bib_coupling_similarity_matrix[i][j] > 0.99:\n",
    "            row_indices.append(i)\n",
    "            col_indices.append(j)\n",
    "            \n",
    "            \n",
    "for idx in range(len(row_indices)):\n",
    "    row = row_indices[idx]\n",
    "    col = col_indices[idx]\n",
    "    year_similarity_array.append(year_similarity_matrix[row][col])\n",
    "    print(f' Reference count of both papers: {np.sum(adj_matrix[row, :])} Bib-coupling Jaccard similarity: {bib_coupling_similarity_matrix[row][col]} Pmid1:{nodes_array[row]} Pmid1:{nodes_array[col]}')\n",
    "#print(f'citation count of both papers: {np.sum(adj_matrix[:,row])} Pmid1:{nodes_array[row]} Pmid1:{nodes_array[col]}')\n",
    "\n",
    "print(f'Average year similarity {np.average(year_similarity_array)}, total cases: {len(row_indices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239f8b0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452492335Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bib_coupling_similarity_matrix = np.load('bib_coupling_similarity_matrix.npy')\n",
    "cocitation_similarity_matrix = np.load('cocitation_similarity_matrix.npy')\n",
    "\n",
    "#For co_citation of 1\n",
    "first_paper_id = row_indices[3]\n",
    "second_paper_id = col_indices[3]\n",
    "\n",
    "\n",
    "first_node_pmid =  3945365 #nodes_array[first_paper_id]\n",
    "second_node_pmid =  11916530 #nodes_array[second_paper_id]\n",
    "\n",
    "first_reference = []\n",
    "second_reference = []\n",
    "\n",
    "\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if start_node == first_node_pmid:\n",
    "        first_reference.append(end_node)   \n",
    "        \n",
    "    if start_node == second_node_pmid:\n",
    "        second_reference.append(end_node)\n",
    "        \n",
    "\n",
    "print(f'first paper pmid: {first_node_pmid} second paper pmid: {second_node_pmid}. total references: {len(first_reference)} Reference list:\\n')\n",
    "print(first_reference)\n",
    "\n",
    "#Assert two arrays are equal\n",
    "assert np.sum(np.sort(first_reference) - np.sort(second_reference)) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd51e3",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "<h1> Missing grants </h1>\n",
    "<p>Let's see how those samples with 'grant' string but no grant data look like </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483c43a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452544824Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_pmids = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if 'grant' in str(value).lower() and len(metadata_dict[key]['grants']) == 0:\n",
    "        target_pmids.append(key)\n",
    "        \n",
    "print(target_pmids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e1c17",
   "metadata": {},
   "source": [
    "Through better parsing, this number is now very low and negligible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e105ed59",
   "metadata": {},
   "source": [
    "<h1>Missing ISSN</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97099845",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452601220Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3645914', '14066214', '1211972', '10474434', '3429506', '4904464', '4347574', '4713388', '4308132', '4383515', '5119427', '5631946', '5293199', '6377493', '6513295', '290137', '234110', '151982', '544612', '1282098', '1691743', '3043083', '3710688', '4233264', '4871582', '4506117', '4789929', '4286257', '5084293', '5302502', '5624558', '6568438', '1797831']\n"
     ]
    }
   ],
   "source": [
    "target_pmids = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if 'journal' in str(value).lower() and len(metadata_dict[key]['journal']) == 0:\n",
    "        target_pmids.append(key)\n",
    "        \n",
    "print(target_pmids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a11f1f",
   "metadata": {},
   "source": [
    "As we see, even with having a lookup dict and mapping journal_title -> ISSN for pairs that we have both data, doesn't help much to resolve ISSN for cases that we only have journal_title. Only 3 over 2004 samples were resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de5dbb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-06T17:37:59.452654109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Canadian journal of biochemistry and physiology' in journal_title_lookup_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
