{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f303592",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T03:07:12.672098150Z",
     "start_time": "2023-09-15T03:07:10.291540657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98753it [00:00, 874919.69it/s]\n",
      "2004it [00:00, 1204727.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#Load edges and nodes\n",
    "\n",
    "import numpy as np\n",
    "from load_data import *\n",
    "from multiprocessing import Process, Manager, Pool  # Import the Pool class\n",
    "\n",
    "nodes_array, edge_array = assert_edges_are_within_first_cluster()\n",
    "\n",
    "node_lookup_dict = {}\n",
    "\n",
    "min_index = np.amin(nodes_array)\n",
    "max_index = np.amax(nodes_array)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for node in nodes_array:\n",
    "    node_lookup_dict[str(node)] = i\n",
    "    i+= 1\n",
    "    \n",
    "adj_matrix = np.load('adj_matrix.npy')    \n",
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdeaad6",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T03:07:44.385980386Z",
     "start_time": "2023-09-15T03:07:12.653100917Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14695476/14695476 [00:31<00:00, 463712.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#Get dois from the exosome csv\n",
    "#Read the DOIS from the node id\n",
    "\n",
    "from load_data import *\n",
    "\n",
    "import csv\n",
    "\n",
    "csv_file_path = data_dir + 'exosome.csv'\n",
    "doi_lookup_dict = {}\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(csv_file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    dois = {}\n",
    "    for line_number, row in tqdm(enumerate(csv_reader), total = 14695476):  # 'total' is the total number of iterations\n",
    "        \n",
    "        if line_number <min_index or line_number > max_index:\n",
    "            continue\n",
    "            \n",
    "        id = str(row[0])\n",
    "        doi = row[2]\n",
    "        \n",
    "        if id in node_lookup_dict:\n",
    "            doi_lookup_dict[id] = doi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9c810cf",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T03:07:44.387296883Z",
     "start_time": "2023-09-15T03:07:44.385634725Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save\n",
    "import json\n",
    "\n",
    "with open('first_cluster_dois.json', 'w') as json_file:\n",
    "    json.dump(doi_lookup_dict, json_file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f66d16",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T03:09:24.845419035Z",
     "start_time": "2023-09-15T03:07:44.386467783Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 873/2004 [01:40<02:10,  8.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-63315c0892c7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnode_lookup_dict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m     \u001B[0mdoi_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfetch_pmid_from_doi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-63315c0892c7>\u001B[0m in \u001B[0;36mfetch_pmid_from_doi\u001B[0;34m(doi)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mrequest_str\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdoi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrequests\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequest_str\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m     \u001B[0mresponse_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/step_1/lib/python3.6/site-packages/requests/api.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m     \"\"\"\n\u001B[1;32m     74\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 75\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'get'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     76\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/step_1/lib/python3.6/site-packages/requests/api.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0;31m# cases, and look like a memory leak in others.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0msessions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSession\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/step_1/lib/python3.6/site-packages/requests/sessions.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    527\u001B[0m         }\n\u001B[1;32m    528\u001B[0m         \u001B[0msend_kwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msettings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 529\u001B[0;31m         \u001B[0mresp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0msend_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    530\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    531\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mresp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/step_1/lib/python3.6/site-packages/requests/sessions.py\u001B[0m in \u001B[0;36msend\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    643\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    644\u001B[0m         \u001B[0;31m# Send the request\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 645\u001B[0;31m         \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madapter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    646\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    647\u001B[0m         \u001B[0;31m# Total elapsed time of the request (approximately)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/step_1/lib/python3.6/site-packages/requests/adapters.py\u001B[0m in \u001B[0;36msend\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    448\u001B[0m                     \u001B[0mdecode_content\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    449\u001B[0m                     \u001B[0mretries\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_retries\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 450\u001B[0;31m                     \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    451\u001B[0m                 )\n\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/step_1/lib/python3.6/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    719\u001B[0m                 \u001B[0mbody\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbody\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    720\u001B[0m                 \u001B[0mheaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 721\u001B[0;31m                 \u001B[0mchunked\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mchunked\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    722\u001B[0m             )\n\u001B[1;32m    723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/step_1/lib/python3.6/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36m_make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    464\u001B[0m                     \u001B[0;31m# Python 3 (including for exceptions like SystemExit).\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    465\u001B[0m                     \u001B[0;31m# Otherwise it looks like a bug in the code.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 466\u001B[0;31m                     \u001B[0msix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    467\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mSocketTimeout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBaseSSLError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSocketError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    468\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_raise_timeout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout_value\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mread_timeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/step_1/lib/python3.6/site-packages/urllib3/packages/six.py\u001B[0m in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/step_1/lib/python3.6/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36m_make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    459\u001B[0m                 \u001B[0;31m# Python 3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    460\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 461\u001B[0;31m                     \u001B[0mhttplib_response\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetresponse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    462\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mBaseException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    463\u001B[0m                     \u001B[0;31m# Remove the TypeError from the exception chain in\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib64/python3.6/http/client.py\u001B[0m in \u001B[0;36mgetresponse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1344\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1345\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1346\u001B[0;31m                 \u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbegin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1347\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mConnectionError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1348\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib64/python3.6/http/client.py\u001B[0m in \u001B[0;36mbegin\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    305\u001B[0m         \u001B[0;31m# read until we get a non-100 response\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 307\u001B[0;31m             \u001B[0mversion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstatus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreason\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_read_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    308\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mstatus\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mCONTINUE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib64/python3.6/http/client.py\u001B[0m in \u001B[0;36m_read_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    267\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_read_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 268\u001B[0;31m         \u001B[0mline\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_MAXLINE\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"iso-8859-1\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    269\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0m_MAXLINE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    270\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mLineTooLong\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"status line\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib64/python3.6/socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    584\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib64/python3.6/ssl.py\u001B[0m in \u001B[0;36mrecv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m    969\u001B[0m                   \u001B[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001B[0m \u001B[0;34m%\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    970\u001B[0m                   self.__class__)\n\u001B[0;32m--> 971\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnbytes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    972\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    973\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0msocket\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnbytes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflags\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib64/python3.6/ssl.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m    831\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Read on closed or unwrapped SSL socket.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    832\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 833\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sslobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    834\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mSSLError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    835\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mSSL_ERROR_EOF\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msuppress_ragged_eofs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib64/python3.6/ssl.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m    588\u001B[0m         \"\"\"\n\u001B[1;32m    589\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mbuffer\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 590\u001B[0;31m             \u001B[0mv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sslobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    591\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    592\u001B[0m             \u001B[0mv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sslobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#Get PMID by doi\n",
    "\n",
    "import requests\n",
    "import xmltodict\n",
    "\n",
    "doi_dict = {}\n",
    "\n",
    "#Get dois\n",
    "def fetch_pmid_from_doi(doi='10.1073/pnas.0510928103'):\n",
    "\n",
    "    pmid_dict = {}\n",
    "    request_str = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids='\n",
    "    request_str += str(doi)\n",
    "    \n",
    "    response = requests.get(request_str)\n",
    "    response_text = response.text\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        index = response_text.index('pmid=')\n",
    "\n",
    "\n",
    "        pmid_whole = response_text[index:].split(' ')[0]\n",
    "        pmid = int(pmid_whole[:-1].split('\\\"')[1])\n",
    "\n",
    "        return pmid\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "for key in tqdm(node_lookup_dict.keys()):\n",
    "    doi_dict[key] = fetch_pmid_from_doi(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e21bf",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.844981692Z"
    }
   },
   "outputs": [],
   "source": [
    "#It turns out that initial node ids is their PMID\n",
    "\n",
    "\n",
    "for key, value in doi_dict.items():\n",
    "    assert key == value\n",
    "    \n",
    "\n",
    "with open('first_cluster_pmid.json', 'w') as json_file:\n",
    "    json.dump(doi_dict, json_file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce168bb",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T03:09:24.849372262Z",
     "start_time": "2023-09-15T03:09:24.847469623Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('first_cluster_pmid.json', 'r') as json_file:\n",
    "    doi_dict = json.load(json_file)\n",
    "    \n",
    "pmid_dict = doi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2af2d",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T03:09:24.850286412Z",
     "start_time": "2023-09-15T03:09:24.850161658Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save all xmls\n",
    "all_xmls = {}\n",
    "def save_all_xmls(pmid):\n",
    "    wait = 0.25\n",
    "    time.sleep(wait)\n",
    "    pmid_dict = {}\n",
    "    request_str  = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id='\n",
    "    request_str += str(pmid)\n",
    "    \n",
    "    response = requests.get(request_str)\n",
    "    response_text = response.text\n",
    "    \n",
    "    try:\n",
    "        xml_dict = xmltodict.parse(response_text)\n",
    "        \n",
    "    except:\n",
    "        time.sleep(2 * wait)\n",
    "        return fetch_metadata_from_pmid(pmid)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "# for id in tqdm(pmid_dict):\n",
    "#     all_xmls[id] = save_all_xmls(id)\n",
    "\n",
    "with open('first_cluster_xmls.json', 'w') as json_file:\n",
    "    json.dump(all_xmls, json_file, indent=4)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9485977d",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.852342912Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load all XMLs\n",
    "import json\n",
    "\n",
    "with open('first_cluster_xmls.json', 'r') as json_file:\n",
    "    all_xmls = json.load(json_file)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29decb9",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.879996702Z"
    }
   },
   "outputs": [],
   "source": [
    "#Mispelled\n",
    "import time\n",
    "import requests\n",
    "import xmltodict\n",
    "\n",
    "parsed_error = []\n",
    "dict_of_interest = {'keyword': 0, \"grantlist\": 0, \"meshheadinglist\": 0, \"chemicallist\":0, 'datecompleted': 0, 'journal': 0}\n",
    "journal_title_lookup_dict = {}\n",
    "journal_ISSN_lookup_dict = {}\n",
    "\n",
    "def fetch_metadata_from_pmid(response_text, idx, pmid): \n",
    "    global dict_of_interest\n",
    "    mesh_headings = []\n",
    "    grants = []\n",
    "    year = \"\"\n",
    "    journal_ISSN = \"\"\n",
    "    abstract = \"\"\n",
    "    chemical_list = []\n",
    "    meta_data = {}\n",
    "    journal_title = \"\"\n",
    "    pub_year = \"\"\n",
    "    keyword_list = []\n",
    "\n",
    "    try:\n",
    "        xml_dict = xmltodict.parse(response_text)\n",
    "        if len(str(xml_dict))< 300:\n",
    "            parsed_error.append(pmid)\n",
    "            raise 'no data'\n",
    "        \n",
    "        for key_of_interest in dict_of_interest.keys():\n",
    "            if key_of_interest in str(xml_dict).lower():\n",
    "\n",
    "                dict_of_interest[key_of_interest] += 1\n",
    "\n",
    "        #Date completed or revised?\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation'].keys())\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if 'DateCompleted' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "                if 'Year' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['DateCompleted']:\n",
    "                    year = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['DateCompleted']['Year']\n",
    "\n",
    "            else:\n",
    "    #             print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation'])\n",
    "                  pass\n",
    "    \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        \n",
    "        try:\n",
    "            if 'JournalIssue' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                if 'PubDate' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']:\n",
    "                    if 'Year' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']:\n",
    "                        pub_year = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "                        \n",
    "            if 'ISSN' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                journal_ISSN = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['ISSN']['#text'] \n",
    "            \n",
    "            if 'Title' in  xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                journal_title = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['Title']\n",
    "                \n",
    "            if len(journal_ISSN) > 0 and len(journal_title)>0:\n",
    "                journal_ISSN_lookup_dict[journal_ISSN] = journal_title\n",
    "                journal_title_lookup_dict[journal_title] = journal_ISSN\n",
    "                \n",
    "            elif len(journal_ISSN) == 0:\n",
    "                if journal_title in journal_title_lookup_dict:\n",
    "                    journal_ISSN = journal_title_lookup_dict[journal_title]\n",
    "                    \n",
    "            elif len(journal_title) == 0:\n",
    "                if journal_ISSN in journal_ISSN_lookup_dict:\n",
    "                    journal_title = journal_ISSN_lookup_dict[journal_ISSN]                \n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e, pmid)\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']))\n",
    "        \n",
    "        \n",
    "        #Grant\n",
    "        #Very few grants don't have grant id's\n",
    "        #Grant institute could also be relevant\n",
    "        #TODO: collect missing grant id\n",
    "\n",
    "        try:\n",
    "            if 'GrantList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']:\n",
    "                if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']) == list:\n",
    "                    for grant in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']:\n",
    "                        if 'GrantID' in grant:\n",
    "                            grants.append((grant['GrantID']))\n",
    "                            \n",
    "                else:\n",
    "                     grants.append(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']['GrantID'])\n",
    "                     pass   \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant'][0])\n",
    "        \n",
    "        #MeSH heading  \n",
    "        \n",
    "        try:\n",
    "              if 'KeywordList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "#                     print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList'].keys())\n",
    "#                     print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword'])\n",
    "#                     print(type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']))\n",
    "#                     print(type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']))\n",
    "                    if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']) == list:\n",
    "                        for key_word in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']:\n",
    "                            keyword_list.append(key_word['#text'])\n",
    "                    \n",
    "                    else:\n",
    "                        keyword_list.append(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']['#text'])\n",
    "                    \n",
    "\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']))\n",
    "        \n",
    "            #DescriptorName? Is a term\n",
    "            if 'MeshHeadingList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "                if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']) == list:\n",
    "                    for mesh in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']:\n",
    "    #                 print(mesh['DescriptorName'])\n",
    "    #                 print(mesh['DescriptorName']['@UI'])\n",
    "    #                 print(mesh['DescriptorName']['#text'])\n",
    "    \n",
    "                        if '@Type' in mesh['DescriptorName'].keys() and mesh['DescriptorName']['@Type'] == 'Geographic':\n",
    "                            continue\n",
    "                \n",
    "                \n",
    "                        mesh_headings.append((mesh['DescriptorName']['@UI']))\n",
    "            \n",
    "                else:\n",
    "                    mesh = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']['DescriptorName']\n",
    "                    if  (not '@Type' in mesh.keys() or mesh['@Type'] != 'Geographic'):\n",
    "                        mesh_headings.append((xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']['DescriptorName']['@UI']))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        try:\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']))\n",
    "        \n",
    "            #DescriptorName? Is a term\n",
    "            if 'ChemicalList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "#                print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']\n",
    "# #                      ['ChemicalList']['Chemical']['NameOfSubstance'])\n",
    "               #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']))\n",
    "               if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']) == list:\n",
    "                   for substance in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']:\n",
    "                            chemical_list.append(substance['NameOfSubstance']['@UI'])\n",
    "                        \n",
    "               else:\n",
    "                   chemical_list.append(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']['NameOfSubstance']['@UI'])\n",
    "                #print(chemical_list)\n",
    "                \n",
    "               \n",
    "              \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "        #References and history\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['PubmedData'].keys())\n",
    "\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    if len(year) == 0:\n",
    "        year = pub_year\n",
    "        \n",
    "    meta_data = {'mesh': mesh_headings, 'grants': grants, 'year': year, 'journal_ISSN': journal_ISSN,'journal_title': journal_title,'chemical' : chemical_list, 'pub_year': pub_year, 'keyword': keyword_list} \n",
    "        \n",
    "    return meta_data\n",
    "\n",
    "metadata_dict = {}\n",
    "\n",
    "# [166, 719, 1672, 1918] odd\n",
    "\n",
    "idx = 0\n",
    "for key, value in tqdm(all_xmls.items()):\n",
    "    metadata_dict[key] = fetch_metadata_from_pmid(value, idx, key)\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "\n",
    "print(f'failed parse {len(parsed_error)} which are: {parsed_error}')\n",
    "print(f'Occured keyword dict: {dict_of_interest}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ef219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-15T03:09:24.912645981Z",
     "start_time": "2023-09-15T03:09:24.880214211Z"
    }
   },
   "outputs": [],
   "source": [
    "from pybliometrics.scopus import AbstractRetrieval\n",
    "import tqdm\n",
    "pmid_authors_dict = {}\n",
    "i = 0\n",
    "failed_authors = []\n",
    "for pmid, value in doi_lookup_dict.items():\n",
    "    if i>1 and i%20 == 0:\n",
    "        print(f'{i//20}% failed: {len(failed_authors)} success: {i-len(failed_authors)}')\n",
    "    i+=1\n",
    "    try:\n",
    "        ab = AbstractRetrieval(value)\n",
    "        author_id_dict = {}\n",
    "        for author in ab.authors:\n",
    "            author_id_dict[author.auid] = 1\n",
    "\n",
    "        pmid_authors_dict[pmid] = author_id_dict\n",
    "        \n",
    "    except:\n",
    "        failed_authors.append(value)\n",
    "#     doi_lookup_dict[key]\n",
    "#     ab = AbstractRetrieval(\"10.1016/j.softx.2019.100263\")\n",
    "#     print(key)\n",
    "#     print(node_lookup_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273e1a7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.880397185Z"
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('pmid_authors_dict.json', 'w') as json_file:\n",
    "#     json.dump(doi_lookup_dict, json_file, indent=4)\n",
    "    \n",
    "x = np.zeros((2004,2004))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb6921",
   "metadata": {},
   "source": [
    "Turns out that out of 2004 papers of CEN cluster1, 11 of those don't have any metadata. That leaves us with 1993 samples. The above dictionary tells us the frequence of keyword occurence. The bellow one tells us the frequency of actual recorded value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2472e",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.880627609Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Count incomplete data\n",
    "\n",
    "features = {'keyword':0, 'mesh': 0, 'grants': 0, 'year': 0, 'journal_title': 0, \"journal_ISSN\":0, 'chemical' : 0, 'pub_year': 0}\n",
    "#Mesh terms are sometimes unrelated\n",
    "\n",
    "mesh_length = []\n",
    "chemical_length = []\n",
    "\n",
    "for feature in features.keys():\n",
    "    for _, meta in metadata_dict.items():\n",
    "        if len(meta[feature]) > 0:\n",
    "            features[feature] += 1\n",
    "            if feature == 'mesh':\n",
    "                mesh_length.append(len(meta[feature]))\n",
    "            \n",
    "            elif feature == 'chemical':\n",
    "                chemical_length.append(len(meta[feature]))\n",
    "                \n",
    "            \n",
    "print(\"recorded features \", features)\n",
    "print(f\"avg mesh length: {np.mean(mesh_length)} std: {np.std(mesh_length)}\" )\n",
    "print(f\"avg chemical length: {np.mean(chemical_length)} std: {np.std(chemical_length)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0c402",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.881645143Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('first_cluster_metadata.json', 'w') as json_file:\n",
    "    json.dump(metadata_dict, json_file, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b161bb",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.887264366Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the metadata file\n",
    "with open('first_cluster_metadata.json', 'r') as json_file:\n",
    "    metadata_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9512e5",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.889729734Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_dict_similarity(dict_1, dict_2, mode ): \n",
    "    similarity = 0\n",
    "    modes = ['number_of_common_terms', 'jaccard']\n",
    "    \n",
    "    assert mode in modes\n",
    "    \n",
    "    for key in dict_1:\n",
    "            if key in dict_2:\n",
    "                similarity += 1\n",
    "                \n",
    "    \n",
    "    if mode == 'number_of_common_terms':\n",
    "        pass\n",
    "       \n",
    "    elif mode == 'jaccard':\n",
    "        similarity = similarity / (len(dict_1) + len(dict_2))\n",
    "        \n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec54f8d",
   "metadata": {},
   "source": [
    "<h1>Checking for same grants</h1> <p> Let's simply record number of same grants. It turns out no two papers have same grant</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7e782",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.892204771Z"
    }
   },
   "outputs": [],
   "source": [
    "#Calculate similarity between two pmid's\n",
    "#Lets call it metric for now. This is unnormalized\n",
    "def calculate_similarity(pmid1, pmid2, feature):\n",
    "    \n",
    "    metric = 0\n",
    "    pmid1 = str(pmid1)\n",
    "    pmid2 = str(pmid2)\n",
    "    \n",
    "    assert feature in ['keyword','year', 'mesh', 'chemical', 'co-citation', 'bib-coupling', 'grants', 'journal']\n",
    "    if feature == 'year':             \n",
    "                year1 = metadata_dict[pmid1][feature]\n",
    "                year2 = metadata_dict[pmid2][feature]\n",
    "                \n",
    "                if len(year1) > 0 and len(year2)>0:\n",
    "                    metric = np.abs(int(year1)-int(year2))\n",
    "                \n",
    "                else:\n",
    "                    metric = -1\n",
    "                    \n",
    "    if feature == 'journal':\n",
    "        issn1 = metadata_dict[pmid1]['journal_ISSN']\n",
    "        issn2 = metadata_dict[pmid2]['journal_ISSN']\n",
    "        \n",
    "        title1 = metadata_dict[pmid1]['journal_title']\n",
    "        title2 = metadata_dict[pmid2]['journal_title']\n",
    "        \n",
    "        \n",
    "        if len(issn1)>0 and len(issn2)>0:\n",
    "            metric = int(issn1==issn2)\n",
    "        \n",
    "        \n",
    "  \n",
    "        elif len(title1)>0 and len(title2)>0:\n",
    "            metric = int(title1==title2)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            metric = -1\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    #They are both treated the same way ...\n",
    "    elif feature == 'mesh' or feature == 'chemical':\n",
    "        common_terms = 0\n",
    "        terms_1 = {}\n",
    "        terms_2 = {}\n",
    "        \n",
    "        for term in metadata_dict[pmid1][feature]:\n",
    "            terms_1[term[0]] = 1\n",
    "    \n",
    "        for term in metadata_dict[pmid2][feature]:\n",
    "            terms_2[term[0]] = 1\n",
    "        \n",
    "        if len(terms_1) > 0 and len(terms_2)>0:\n",
    "            metric = calculate_dict_similarity(terms_1, terms_2, 'jaccard')\n",
    "                    \n",
    "        else:\n",
    "            metric = -1\n",
    "            \n",
    "    elif feature == 'bib-coupling':\n",
    "        node_1 = node_lookup_dict[pmid1]\n",
    "        node_2 = node_lookup_dict[pmid2]\n",
    "        \n",
    "        common = np.dot(adj_matrix[node_1,:], adj_matrix[node_2, :])\n",
    "        denom = np.sum(adj_matrix[node_1, :]) + np.sum(adj_matrix[node_2, :]) - common\n",
    "        \n",
    "        if denom > 0 :\n",
    "            metric = (common / denom)*(1-np.exp(-0.1*denom))\n",
    "        \n",
    "    \n",
    "    elif feature == 'co-citation':\n",
    "        node_1 = node_lookup_dict[pmid1]\n",
    "        node_2 = node_lookup_dict[pmid2]\n",
    "        \n",
    "        common = np.dot(adj_matrix[: , node_1], adj_matrix[: , node_2])\n",
    "        denom = np.sum(adj_matrix[:, node_1]) + np.sum(adj_matrix[:, node_2]) - common\n",
    "        \n",
    "        if denom > 0 :\n",
    "            metric = (common / denom)*(1-np.exp(-0.1*denom))\n",
    "            \n",
    "    \n",
    "    elif feature == 'grants':\n",
    "        grants_1 = metadata_dict[pmid1]['grants']\n",
    "        grants_2 = metadata_dict[pmid2]['grants']\n",
    "       \n",
    "        \n",
    "        for first in grants_1:\n",
    "            for second in grants_2:\n",
    "                if first == second:\n",
    "                    metric +=1\n",
    "                    \n",
    "    elif feature == 'keyword':\n",
    "        keyword_1 = metadata_dict[pmid1]['keyword']\n",
    "        keyword_2 = metadata_dict[pmid2]['keyword']\n",
    "       \n",
    "        if len(keyword_1) == 0 or len(keyword_2)== 0:\n",
    "            metric = -1\n",
    "        \n",
    "        else:\n",
    "            common = 0\n",
    "            for first in keyword_1:\n",
    "                for second in keyword_2:\n",
    "                    if first == second:\n",
    "                        common +=1\n",
    "\n",
    "\n",
    "            metric = common/(len(keyword_1)+len(keyword_2)-common)\n",
    "\n",
    "                    \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c7268",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.894664749Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_three_hop_similarity():\n",
    "    #Is currently inefficient as relies on matrix multiplication\n",
    "    # We use undirected edges\n",
    "    aggregated_three_hop_similarity = np.zeros_like(adj_matrix)\n",
    "    undirected_adj_matrix = adj_matrix + adj_matrix.transpose()\n",
    "    second_hop_distance = np.matmul(undirected_adj_matrix, undirected_adj_matrix)\n",
    "    \n",
    "    three_hop_distance = np.matmul(second_hop_distance, undirected_adj_matrix)\n",
    "    \n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        for j in range(i):\n",
    "            similarity = 0\n",
    "            if undirected_adj_matrix[i][j] == 1:\n",
    "                similarity = 1\n",
    "            \n",
    "            elif second_hop_distance[i][j] > 0:\n",
    "                similarity = 2/3\n",
    "            \n",
    "            elif three_hop_distance[i][j] > 0:\n",
    "                similarity = 1/3\n",
    "            \n",
    "            aggregated_three_hop_similarity[i][j] = similarity\n",
    "    \n",
    "    aggregated_three_hop_similarity = aggregated_three_hop_similarity + aggregated_three_hop_similarity.transpose()\n",
    "    return aggregated_three_hop_similarity\n",
    "\n",
    "aggregated_three_hop_similarity = calculate_three_hop_similarity()\n",
    "np.save('aggregated_three_hop_similarity.npy', aggregated_three_hop_similarity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748c102",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.896521292Z"
    }
   },
   "outputs": [],
   "source": [
    "#all mesh terms are tuples, so we use UI's as they are standard\n",
    "\n",
    "ones = 0\n",
    "twos = 0\n",
    "\n",
    "for key, value in metadata_dict.items():\n",
    "    if len(value['mesh']) >0:\n",
    "        for mesh in value['mesh']:\n",
    "            if len(mesh) == 1:\n",
    "                ones += 1\n",
    "            \n",
    "            elif len(mesh) == 2:\n",
    "                twos += 1\n",
    "            \n",
    "            else:\n",
    "                raise 'error'\n",
    "            \n",
    "print(f'Mesh terms with one entires {ones}')\n",
    "print(f'Mesh terms with two entires {twos}')\n",
    "print(f'Example of a mesh: {mesh}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae28d79",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.898350404Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "def report_stats(vals, title, caption, y_axis):\n",
    "    filtered_val = []\n",
    "    for val in vals:\n",
    "        if val != -1:\n",
    "            filtered_val.append(val)\n",
    "            \n",
    "    median = np.median(filtered_val)\n",
    "    mean = np.mean(filtered_val)\n",
    "    min = np.amin(filtered_val)\n",
    "    max = np.amax(filtered_val)\n",
    "    total = np.sum(filtered_val)\n",
    "    \n",
    "    font = {'family' : 'Times New Roman',\n",
    "#         'weight' : 'bold',\n",
    "        'size'   : 18}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "#     fig.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(vals)\n",
    "    \n",
    "    labels = [title]\n",
    "\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "    ax.set_title(f'{title}')\n",
    "    ax.set_xlabel(f'{caption}')\n",
    "    ax.set_ylabel(f'{y_axis}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(f'median {median} mean {mean} min {min} max {max} total {total}')\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_similarity_matrix(feature):\n",
    "    similarity_matrix = np.zeros((len(nodes_array), len(nodes_array))) \n",
    "    \n",
    "    for i in tqdm(range(similarity_matrix.shape[0])):\n",
    "        for j in range(i):\n",
    "            pmid1 = str(nodes_array[i])\n",
    "            pmid2 = str(nodes_array[j])            \n",
    "            similarity_matrix[i][j] = calculate_similarity(pmid1, pmid2, feature)\n",
    "        \n",
    "    similarity_matrix = similarity_matrix + similarity_matrix.transpose()\n",
    "    \n",
    "    #Year is the only feature that is initially distance and needs to be similarity\n",
    "    if feature == 'year':\n",
    "        similarity_matrix = np.ones_like(similarity_matrix) - similarity_matrix/np.amax(similarity_matrix)\n",
    "        similarity_matrix = np.where(similarity_matrix > 1, -1, similarity_matrix)\n",
    "        \n",
    "    return similarity_matrix\n",
    "\n",
    "# year_similarity_matrix = calculate_similarity_matrix('year')\n",
    "# np.save('year_similarity_matrix.npy', year_similarity_matrix)\n",
    "\n",
    "# mesh_similarity_matrix = calculate_similarity_matrix('mesh')\n",
    "# np.save('mesh_similarity_matrix.npy', mesh_similarity_matrix)\n",
    "\n",
    "\n",
    "bib_coupling_similarity_matrix = calculate_similarity_matrix('bib-coupling')\n",
    "np.save('bib_coupling_similarity_matrix.npy', bib_coupling_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "cocitation_similarity_matrix = calculate_similarity_matrix('co-citation')\n",
    "np.save('cocitation_similarity_matrix.npy', cocitation_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# chemical_similarity_matrix = calculate_similarity_matrix('chemical')\n",
    "# np.save('chemical_similarity_matrix.npy', chemical_similarity_matrix)\n",
    "\n",
    "\n",
    "# grants_similarity_matrix = calculate_similarity_matrix('grants')\n",
    "# np.save('grants_similarity_matrix.npy', grants_similarity_matrix)\n",
    "\n",
    "\n",
    "# journal_similarity_matrix = calculate_similarity_matrix('journal')\n",
    "# np.save('journal_similarity_matrix.npy', grants_similarity_matrix)\n",
    "\n",
    "\n",
    "# keyword_similarity_matrix = calculate_similarity_matrix('keyword')\n",
    "# np.save('keyword_similarity_matrix.npy', keyword_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8626da",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.900165679Z"
    }
   },
   "outputs": [],
   "source": [
    "cocitation_matrix = np.load('cocitation_matrix.npy')\n",
    "bib_coupling_matrix = np.load('bib_coupling_matrix.npy')\n",
    "\n",
    "cocitation_matrix /= np.amax(cocitation_matrix)\n",
    "bib_coupling_matrix /= np.amax(bib_coupling_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad3cec",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.902106491Z"
    }
   },
   "outputs": [],
   "source": [
    "#Assuming a square matrix, report statistics\n",
    "def report_matrix_stats(matrix, title, caption, y_axis):\n",
    "    n = matrix.shape[0]\n",
    "    all_values = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            #Remove negatives as they are flags\n",
    "            if matrix[i][j] > -1:\n",
    "                all_values.append(matrix[i][j])\n",
    "    \n",
    "    report_stats(all_values, title, caption, y_axis)\n",
    "\n",
    "# title = 'Year similarity boxplot'\n",
    "# caption = 'Year similarity value is defined by year difference devided by maximum difference'\n",
    "# y_axis = 'Year similarity'\n",
    "# report_matrix_stats(year_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "# title = 'Jaccard mesh similarity boxplot'\n",
    "# caption = 'Jaccard mesh similarity value is defined by \\n the size of union of two mesh terms devided by the size of their intersection'\n",
    "# y_axis = 'Jaccard mesh similarity'\n",
    "# report_matrix_stats(mesh_similarity_matrix,  title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "# title = 'Jaccard chemical similarity boxplot'\n",
    "# caption = 'Jaccard chemical similarity value is defined by \\n the size of union of two chemical lists devided by the size of their intersection'\n",
    "# y_axis = 'Jaccard chemical similarity'\n",
    "# report_matrix_stats(chemical_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "title = 'Discounted Jaccard co-citation similarity boxplot'\n",
    "caption = 'Discounted Jaccard co-citation similarity value is defined by \\n the size of union of inward citation list devided by the size of their intersection also multiplied by the exponential discount factor'\n",
    "y_axis = 'Discounted Jaccard co-citation similarity'\n",
    "report_matrix_stats(cocitation_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "# title = 'Max-normalized co-citation similarity boxplot'\n",
    "# caption = 'Max-normalized co-citation similarity value is defined by \\n the frequency of co-citation devided by maximum co-citation value observed'\n",
    "# y_axis = 'Max-normalized co-citation similarity'\n",
    "# report_matrix_stats(cocitation_matrix,  title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "title = 'Discounted Jaccard bib-coupling similarity boxplot'\n",
    "caption = 'Discounted Jaccard bib-coupling similarity value is defined by \\n the size of union of outward citation list devided by the size of their intersection also multiplied by the exponential discount factor'\n",
    "y_axis = 'Discounted Jaccard bib-coupling similarity'\n",
    "report_matrix_stats(bib_coupling_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "# title = 'Max-normalized bib-coupling similarity boxplot'\n",
    "# caption = 'Max-normalized bib-coupling similarity value is defined by \\n the frequency of bib-coupling devided by maximum bibcoupling value observed'\n",
    "# y_axis = 'Max-normalized bib-coupling similarity'\n",
    "# report_matrix_stats(bib_coupling_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "# title = 'Three-hop range similarity boxplot'\n",
    "# caption = 'Three-hop range similarity value is defined in an undirected graph, \\n as 1 if two nodes are neighbours, 2/3 if their distance is 2, 1/3 if their distance is 3 \\n and 0 otherwise'\n",
    "# y_axis = 'Three-hop range similarity'\n",
    "# report_matrix_stats(aggregated_three_hop_similarity, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "# title = 'Grant similarity boxplot'\n",
    "# caption = 'Grant similarity value is defined as 1 if two nodes have listed the same grant and 0 otherwise'\n",
    "# y_axis = 'Grant similarity'\n",
    "# report_matrix_stats(grants_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "# title = 'Journal similarity boxplot'\n",
    "# caption = 'Journal similarity value is defined as 1 if two nodes are published in a same journal and 0 otherwise'\n",
    "# y_axis = 'Journal similarity'\n",
    "# report_matrix_stats(journal_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "# title = 'Jaccard keyword similarity boxplot'\n",
    "# caption = 'Keyword similarity value is defined as jaccard similarity of two keywords if existed \\n else this similarity is not taken into account'\n",
    "# y_axis = 'Keyword similarity'\n",
    "# report_matrix_stats(keyword_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9a7ed",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.904831568Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(grants_similarity_matrix))\n",
    "\n",
    "for key, value in metadata_dict.items():\n",
    "    if len(value['grants']) > 0:\n",
    "        print(value['grants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea75bd3",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.907323166Z"
    }
   },
   "outputs": [],
   "source": [
    "#Verify that those entries without grant actually do not have grants\n",
    "sample_list_of_pmid_without_grant = []\n",
    "without_grants_doi = []\n",
    "i = 0\n",
    "with_grants = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if 'grant' in str(value).lower():\n",
    "        with_grants.append(key)\n",
    "    \n",
    "    else:\n",
    "        sample_list_of_pmid_without_grant.append(key)\n",
    "        without_grants_doi.append(doi_lookup_dict[key])\n",
    "        \n",
    "    i+= 1\n",
    "    \n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "print('Statistics for first 10 papers:\\n')\n",
    "print(f'DOIs without grant: {without_grants_doi[0:10]}')\n",
    "print(f'PMIDs without grant: {sample_list_of_pmid_without_grant[0:10]}\\n')\n",
    "\n",
    "print(f'PMID of papers with grants {with_grants}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd746ba",
   "metadata": {},
   "source": [
    "<h1>Example</h1>\n",
    "An example of bib-couple and co-citation calculation for two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784fe88",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.909303802Z"
    }
   },
   "outputs": [],
   "source": [
    "first_node_id = 0\n",
    "first_node_pmid = nodes_array[first_node_id]\n",
    "\n",
    "second_node_id = 50\n",
    "second_node_pmid = nodes_array[second_node_id]\n",
    "\n",
    "first_reference = {}\n",
    "first_cited = {}\n",
    "\n",
    "second_reference = {}\n",
    "second_cited = {}\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if start_node == first_node_pmid:\n",
    "        first_reference[end_node] = 1\n",
    "    if end_node == first_node_pmid:\n",
    "        first_cited[start_node] = 1     \n",
    "        \n",
    "    \n",
    "    if start_node == second_node_pmid:\n",
    "        second_reference[end_node] = 1\n",
    "    if end_node == second_node_pmid:\n",
    "        second_cited[start_node] = 1       \n",
    "\n",
    "print(f'recorded bib-coupling: {bib_coupling_matrix[first_node_id][second_node_id]}')\n",
    "print(f'crossed checked bib-coupling: {calculate_dict_similarity(first_reference, second_reference, mode = \"number_of_common_terms\")}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'recorded co-citation: {cocitation_matrix[first_node_id][second_node_id]}')\n",
    "print(f'crossed checked co-citation: {calculate_dict_similarity(first_cited, second_cited, mode = \"number_of_common_terms\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d49b37",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "<h1>Missing ISSN</h1>\n",
    "Finding xml's without ISSN. Any journal info?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea48bb",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.911273939Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_issn = []\n",
    "for key, value in all_xmls.items():\n",
    "    if not 'issn' in str(value).lower():\n",
    "        missing_issn.append(key)\n",
    "        \n",
    "print(f'missing ISSN pmids {missing_issn}')\n",
    "print(len(missing_issn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab282043",
   "metadata": {},
   "source": [
    "<h1>Examples of co-citation similarity of 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4b7d8",
   "metadata": {},
   "source": [
    "Interestingly, for all those of paris with jaccard_cocitation = 1, they have been only cited once! As we see below, citation counts of all of those pairs is 1. This could falsly inflate co_citation similarity. This will likely be the case with new publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff64255",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-09-15T03:09:24.914317185Z",
     "start_time": "2023-09-15T03:09:24.912962135Z"
    }
   },
   "outputs": [],
   "source": [
    "year_similarity_matrix = np.load('year_similarity_matrix.npy')\n",
    "year_similarity_array = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "for i in range(cocitation_similarity_matrix.shape[0]):\n",
    "    for j in range(i):\n",
    "        if cocitation_similarity_matrix[i][j] > 0.99:\n",
    "            row_indices.append(i)\n",
    "            col_indices.append(j)\n",
    "\n",
    "for idx in range(len(row_indices)):\n",
    "    row = row_indices[idx]\n",
    "    col = col_indices[idx]\n",
    "    year_similarity_array.append(year_similarity_matrix[row][col])\n",
    "    print(f'citation count of both papers: {np.sum(adj_matrix[:,row])} cocitation Jaccard similarity: {cocitation_similarity_matrix[row][col]} Pmid1:{nodes_array[row]} Pmid1:{nodes_array[col]}')\n",
    "\n",
    "\n",
    "print(f'Average year similarity {np.average(year_similarity_array)} total cases {len(row_indices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b73db1",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.914790786Z"
    }
   },
   "outputs": [],
   "source": [
    "bib_coupling_similarity_matrix = np.load('bib_coupling_similarity_matrix.npy')\n",
    "cocitation_similarity_matrix = np.load('cocitation_similarity_matrix.npy')\n",
    "\n",
    "#For co_citation of 1\n",
    "first_paper_id = np.unravel_index(cocitation_similarity_matrix.argmax(), cocitation_similarity_matrix.shape)[0]\n",
    "second_paper_id = np.unravel_index(cocitation_similarity_matrix.argmax(), cocitation_similarity_matrix.shape)[1]\n",
    "\n",
    "first_node_pmid =  nodes_array[first_paper_id]\n",
    "second_node_pmid =  nodes_array[second_paper_id]\n",
    "\n",
    "first_cited = []\n",
    "second_cited = []\n",
    "\n",
    "\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if end_node == first_node_pmid:\n",
    "        first_cited.append(start_node)   \n",
    "        \n",
    "    if end_node == second_node_pmid:\n",
    "        second_cited.append(start_node)\n",
    "        \n",
    "\n",
    "print(f'first paper pmid: {first_node_pmid} second paper pmid: {second_node_pmid}. Citing list:\\n')\n",
    "print(first_cited)\n",
    "\n",
    "#Assert two arrays are equal\n",
    "assert np.sum(np.sort(first_cited) - np.sort(second_cited)) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f863b",
   "metadata": {},
   "source": [
    "<h1>Examples of bib-coupling similarity of 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c1dcd",
   "metadata": {},
   "source": [
    "Here the situation is differen. As we expect, reference list is typically larger than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc92ca",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.916628824Z"
    }
   },
   "outputs": [],
   "source": [
    "row_indices, col_indices = np.where(bib_coupling_similarity_matrix == 1)\n",
    "year_similarity_array = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "\n",
    "for i in range(bib_coupling_similarity_matrix.shape[0]):\n",
    "    for j in range(i):\n",
    "        if bib_coupling_similarity_matrix[i][j] > 0.99:\n",
    "            row_indices.append(i)\n",
    "            col_indices.append(j)\n",
    "            \n",
    "            \n",
    "for idx in range(len(row_indices)):\n",
    "    row = row_indices[idx]\n",
    "    col = col_indices[idx]\n",
    "    year_similarity_array.append(year_similarity_matrix[row][col])\n",
    "    print(f' Reference count of both papers: {np.sum(adj_matrix[row, :])} Bib-coupling Jaccard similarity: {bib_coupling_similarity_matrix[row][col]} Pmid1:{nodes_array[row]} Pmid1:{nodes_array[col]}')\n",
    "#print(f'citation count of both papers: {np.sum(adj_matrix[:,row])} Pmid1:{nodes_array[row]} Pmid1:{nodes_array[col]}')\n",
    "\n",
    "print(f'Average year similarity {np.average(year_similarity_array)}, total cases: {len(row_indices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239f8b0",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.918473365Z"
    }
   },
   "outputs": [],
   "source": [
    "bib_coupling_similarity_matrix = np.load('bib_coupling_similarity_matrix.npy')\n",
    "cocitation_similarity_matrix = np.load('cocitation_similarity_matrix.npy')\n",
    "\n",
    "#For co_citation of 1\n",
    "first_paper_id = row_indices[3]\n",
    "second_paper_id = col_indices[3]\n",
    "\n",
    "\n",
    "first_node_pmid =  3945365 #nodes_array[first_paper_id]\n",
    "second_node_pmid =  11916530 #nodes_array[second_paper_id]\n",
    "\n",
    "first_reference = []\n",
    "second_reference = []\n",
    "\n",
    "\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if start_node == first_node_pmid:\n",
    "        first_reference.append(end_node)   \n",
    "        \n",
    "    if start_node == second_node_pmid:\n",
    "        second_reference.append(end_node)\n",
    "        \n",
    "\n",
    "print(f'first paper pmid: {first_node_pmid} second paper pmid: {second_node_pmid}. total references: {len(first_reference)} Reference list:\\n')\n",
    "print(first_reference)\n",
    "\n",
    "#Assert two arrays are equal\n",
    "assert np.sum(np.sort(first_reference) - np.sort(second_reference)) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd51e3",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "<h1> Missing grants </h1>\n",
    "<p>Let's see how those samples with 'grant' string but no grant data look like </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483c43a",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.920330258Z"
    }
   },
   "outputs": [],
   "source": [
    "target_pmids = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if 'grantlist' in str(value).lower() and len(metadata_dict[key]['grants']) == 0:\n",
    "        target_pmids.append(key)\n",
    "        \n",
    "print(target_pmids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e1c17",
   "metadata": {},
   "source": [
    "Through better parsing, this number is now very low!\n",
    "Now the case of this 3:\n",
    "<html>\n",
    "<head>\n",
    "    <title>PMID and Reason for Missing</title>\n",
    "</head>\n",
    "<body>\n",
    "    <table border=\"1\">\n",
    "        <tr>\n",
    "            <th>PMID</th>\n",
    "            <th>Reason for Missing</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1971008</td>\n",
    "            <td>No grant Id, only agency and country</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e105ed59",
   "metadata": {},
   "source": [
    "<h1>Missing Mesh</h1> \n",
    "As we verify manually, these 3 articles don't have any mesh term inside them. So, out of 1993 parsable responses, only 3 miss mesh information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97099845",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.922165832Z"
    }
   },
   "outputs": [],
   "source": [
    "target_pmids = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if not'meshheadinglist' in str(value).lower() and len(str(all_xmls[key])) > 250:\n",
    "        target_pmids.append(key)\n",
    "        \n",
    "print(target_pmids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69df6c",
   "metadata": {},
   "source": [
    "<h1>Highly repeated Chemicals </h1>\n",
    "<p> As we can see, in the first cluster, 96.32% of chemical terms are already in MesH terms </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a033c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.923983482Z"
    }
   },
   "outputs": [],
   "source": [
    "inside = 0\n",
    "outside = 0\n",
    "chemicals_not_in_MeSH = []\n",
    "\n",
    "for key, value in tqdm(metadata_dict.items()):\n",
    "    for chemical in value['chemical']:\n",
    "        if chemical in value['mesh']:\n",
    "            inside += 1\n",
    "        \n",
    "        else:\n",
    "            chemicals_not_in_MeSH.append(chemical)\n",
    "            outside += 1\n",
    "\n",
    "print(f'Number of whole chemicals in this cluster {inside + outside}')\n",
    "print(f'Percentage of Chemicals already in MeSH terms {(100*inside/(inside+outside)):.2f}')\n",
    "print(chemicals_not_in_MeSH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d7f1c",
   "metadata": {},
   "source": [
    "<h1>Finding MeSH from UI </h1>\n",
    "<p> Currently we only have UI for MeSH terms or chemicals, but we want their tree structure to find their similarity. As it turns out, we can only get the MeSH tree for MeSH terms, not for chemicals </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b81f95",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.925836749Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import multiprocessing \n",
    "\n",
    "def retrieve_mesh_tree(UI, idx, shared_dict):\n",
    "    url = f\"https://meshb.nlm.nih.gov/record/ui?ui={UI}\"\n",
    "    with requests.Session() as session:\n",
    "        url = f\"https://meshb.nlm.nih.gov/record/ui?ui={UI}\"\n",
    "\n",
    "        response = session.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        text_of_interest = response.text\n",
    "        \n",
    "        try:\n",
    "            treeNumber_idx = text_of_interest.index('treeNumber_0')\n",
    "            uniqueID = text_of_interest.index('Unique ID')\n",
    "            limited_string = text_of_interest[treeNumber_idx:uniqueID]\n",
    "            end_index = limited_string.find('</a>')\n",
    "            counter = 1\n",
    "\n",
    "            while end_index-counter + 1 >= 0:\n",
    "                if limited_string[end_index-counter] == '>':\n",
    "                    shared_dict[UI] = limited_string[end_index-counter + 1:end_index]\n",
    "                    break\n",
    "                counter += 1\n",
    "            \n",
    "        except:\n",
    "            shared_dict[UI] = \"failed\"\n",
    "\n",
    "    else:\n",
    "        shared_dict[UI] = \"failed\"\n",
    "    \n",
    "#     print(shared_dict[UI], idx, \"OVERLORD\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "temp_mesh = {}\n",
    "all_mesh_terms = Manager().dict()\n",
    "\n",
    "lst = []\n",
    "\n",
    "\n",
    "for key, value in metadata_dict.items():\n",
    "    for mesh in value['mesh']:\n",
    "        if not mesh in all_mesh_terms:\n",
    "            temp_mesh[mesh] = 1\n",
    "            \n",
    "\n",
    "for index, mesh in enumerate(temp_mesh.keys()):\n",
    "    lst.append((mesh, index, all_mesh_terms))\n",
    "\n",
    "    \n",
    "CHUNK_SIZE = 50\n",
    "SPLIT = len(lst) // CHUNK_SIZE + 1\n",
    "\n",
    "\n",
    "for i in tqdm(range(SPLIT)):\n",
    "    if i != SPLIT -1:\n",
    "        with multiprocessing.Pool(processes=10) as pool:\n",
    "                pool.starmap(retrieve_mesh_tree, lst[i*CHUNK_SIZE: (i+1) * CHUNK_SIZE])\n",
    "    \n",
    "    else:\n",
    "        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "            pool.starmap(retrieve_mesh_tree, lst[i*CHUNK_SIZE: ])  \n",
    "\n",
    "        \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc791cd",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.927677572Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "mesh_save_dict = {}\n",
    "failed = 0\n",
    "success = 0\n",
    "\n",
    "for key, val in all_mesh_terms.items():\n",
    "    mesh_save_dict[key] = val\n",
    "    \n",
    "    if val == 'failed':\n",
    "        failed += 1\n",
    "    \n",
    "    else:\n",
    "        success += 1\n",
    "    \n",
    "print(success, failed)\n",
    "# with open('all_meshhh_terms.json', 'w') as json_file:\n",
    "#     json.dump(mesh_save_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888cd57",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.929513356Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f9181",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-09-15T03:09:24.931347687Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
