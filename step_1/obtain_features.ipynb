{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f303592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T23:54:52.994609930Z",
     "start_time": "2023-08-31T23:54:50.673926179Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98753it [00:00, 918293.82it/s]\n",
      "2004it [00:00, 1372083.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#Load edges and nodes\n",
    "\n",
    "import numpy as np\n",
    "from load_data import *\n",
    "\n",
    "nodes_array, edge_array = assert_edges_are_within_first_cluster()\n",
    "\n",
    "node_lookup_dict = {}\n",
    "\n",
    "min_index = np.amin(nodes_array)\n",
    "max_index = np.amax(nodes_array)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for node in nodes_array:\n",
    "    node_lookup_dict[str(node)] = i\n",
    "    i+= 1\n",
    "    \n",
    "adj_matrix = np.load('adj_matrix.npy')    \n",
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeaad6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-31T23:54:52.995707034Z"
    },
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 7563694/14695476 [00:16<00:15, 460182.09it/s]"
     ]
    }
   ],
   "source": [
    "#Get dois from the exosome csv\n",
    "#Read the DOIS from the node id\n",
    "\n",
    "from load_data import *\n",
    "\n",
    "import csv\n",
    "\n",
    "csv_file_path = data_dir + 'exosome.csv'\n",
    "doi_lookup_dict = {}\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(csv_file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    dois = {}\n",
    "    for line_number, row in tqdm(enumerate(csv_reader), total = 14695476):  # 'total' is the total number of iterations\n",
    "        \n",
    "        if line_number <min_index or line_number > max_index:\n",
    "            continue\n",
    "            \n",
    "        id = str(row[0])\n",
    "        doi = row[2]\n",
    "        \n",
    "        if id in node_lookup_dict:\n",
    "            doi_lookup_dict[id] = doi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c810cf",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save\n",
    "import json\n",
    "\n",
    "with open('first_cluster_dois.json', 'w') as json_file:\n",
    "    json.dump(doi_lookup_dict, json_file, indent=4)\n",
    "    \n",
    "print(doi_lookup_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f66d16",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get PMID by doi\n",
    "\n",
    "import requests\n",
    "import xmltodict\n",
    "\n",
    "doi_dict = {}\n",
    "\n",
    "#Get dois\n",
    "def fetch_pmid_from_doi(doi='10.1073/pnas.0510928103'):\n",
    "\n",
    "    pmid_dict = {}\n",
    "    request_str = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids='\n",
    "    request_str += str(doi)\n",
    "    \n",
    "    response = requests.get(request_str)\n",
    "    response_text = response.text\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        index = response_text.index('pmid=')\n",
    "\n",
    "\n",
    "        pmid_whole = response_text[index:].split(' ')[0]\n",
    "        pmid = int(pmid_whole[:-1].split('\\\"')[1])\n",
    "\n",
    "        return pmid\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "for key in tqdm(node_lookup_dict.keys()):\n",
    "    doi_dict[key] = fetch_pmid_from_doi(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e21bf",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#It turns out that initial node ids is their PMID\n",
    "\n",
    "\n",
    "for key, value in doi_dict.items():\n",
    "    assert key == value\n",
    "    \n",
    "\n",
    "with open('first_cluster_pmid.json', 'w') as json_file:\n",
    "    json.dump(doi_dict, json_file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce168bb",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('first_cluster_pmid.json', 'r') as json_file:\n",
    "    doi_dict = json.load(json_file)\n",
    "    \n",
    "pmid_dict = doi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2af2d",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save all xmls\n",
    "all_xmls = {}\n",
    "def save_all_xmls(pmid):\n",
    "    wait = 0.25\n",
    "    time.sleep(wait)\n",
    "    pmid_dict = {}\n",
    "    request_str  = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id='\n",
    "    request_str += str(pmid)\n",
    "    \n",
    "    response = requests.get(request_str)\n",
    "    response_text = response.text\n",
    "    \n",
    "    try:\n",
    "        xml_dict = xmltodict.parse(response_text)\n",
    "        \n",
    "    except:\n",
    "        time.sleep(2 * wait)\n",
    "        return fetch_metadata_from_pmid(pmid)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "# for id in tqdm(pmid_dict):\n",
    "#     all_xmls[id] = save_all_xmls(id)\n",
    "\n",
    "with open('first_cluster_xmls.json', 'w') as json_file:\n",
    "    json.dump(all_xmls, json_file, indent=4)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9485977d",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load all XMLs\n",
    "import json\n",
    "\n",
    "with open('first_cluster_xmls.json', 'r') as json_file:\n",
    "    all_xmls = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29decb9",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mispelled\n",
    "import time\n",
    "import requests\n",
    "import xmltodict\n",
    "\n",
    "parsed_error = []\n",
    "dict_of_interest = {'keyword': 0, \"grant\": 0, \"abstract\": 0, \"mesh\": 0, \"chemicallist\":0, 'datecompleted': 0}\n",
    "\n",
    "\n",
    "def fetch_metadata_from_pmid(response_text, idx, pmid): \n",
    "    global dict_of_interest\n",
    "    mesh_headings = []\n",
    "    grants = []\n",
    "    year = \"\"\n",
    "    journal_ISSN = \"\"\n",
    "    abstract = \"\"\n",
    "    chemical_list = []\n",
    "    meta_data = {}\n",
    "    \n",
    "    try:\n",
    "        xml_dict = xmltodict.parse(response_text)\n",
    "        \n",
    "        for key_of_interest in dict_of_interest.keys():\n",
    "            if key_of_interest in str(xml_dict).lower():\n",
    "\n",
    "                dict_of_interest[key_of_interest] += 1\n",
    "\n",
    "        #Date completed or revised?\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation'].keys())\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if 'DateCompleted' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "                if 'Year' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['DateCompleted']:\n",
    "                    year = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['DateCompleted']['Year']\n",
    "\n",
    "            else:\n",
    "    #             print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation'])\n",
    "                  pass\n",
    "    \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            if 'ISSN' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                journal_ISSN = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['ISSN']['#text']\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']))\n",
    "        \n",
    "        \n",
    "        #Grant\n",
    "        #Very few grants don't have grant id's\n",
    "        #Grant institute could also be relevant\n",
    "        #TODO: collect missing grant id\n",
    "\n",
    "        try:\n",
    "            if 'GrantList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']:\n",
    "                for grant in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']:\n",
    "                    if 'GrantID' in grant and 'Agency' in grant:\n",
    "                        grants.append((grant['GrantID'],grant['Agency']))\n",
    "                        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            if 'Abstract' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article'].keys():\n",
    "                if 'AbstractText'  in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Abstract'].keys():\n",
    "                    abstract = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Abstract']['AbstractText']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    " \n",
    "\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant'][0])\n",
    "        \n",
    "        #MeSH heading  \n",
    "        \n",
    "        try:\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']))\n",
    "        \n",
    "            #DescriptorName? Is a term\n",
    "            if 'MeshHeadingList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "                for mesh in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']:\n",
    "    #                 print(mesh['DescriptorName'])\n",
    "    #                 print(mesh['DescriptorName']['@UI'])\n",
    "    #                 print(mesh['DescriptorName']['#text'])\n",
    "                    mesh_headings.append((mesh['DescriptorName']['@UI'], mesh['DescriptorName']['#text']))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        try:\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']))\n",
    "        \n",
    "            #DescriptorName? Is a term\n",
    "            if 'ChemicalList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "#                print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']\n",
    "# #                      ['ChemicalList']['Chemical']['NameOfSubstance'])\n",
    "               #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']))\n",
    "               for substance in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']:\n",
    "                        chemical_list.append(substance['NameOfSubstance']['@UI'])\n",
    "                #print(chemical_list)\n",
    "                \n",
    "               \n",
    "              \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "        #References and history\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['PubmedData'].keys())\n",
    "\n",
    "\n",
    "    except:\n",
    "        parsed_error.append(idx)\n",
    "        pass\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "    meta_data = {'mesh': mesh_headings, 'grants': grants, 'year': year, 'journal': journal_ISSN,\n",
    "                 'abstract': abstract, 'chemical' : chemical_list} \n",
    "        \n",
    "    return meta_data\n",
    "\n",
    "metadata_dict = {}\n",
    "\n",
    "# [166, 719, 1672, 1918] odd\n",
    "\n",
    "idx = 0\n",
    "for key, value in tqdm(all_xmls.items()):\n",
    "    metadata_dict[key] = fetch_metadata_from_pmid(value, idx, key)\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "\n",
    "print(f'failed parse {len(parsed_error)}')\n",
    "print(f'Occured keyword dict: {dict_of_interest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d6597",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2472e",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Count incomplete data\n",
    "\n",
    "features = {'mesh': 0, 'grants': 0, 'year': 0, 'journal': 0, 'abstract': 0, 'chemical' : 0}\n",
    "#Mesh terms are sometimes unrelated\n",
    "\n",
    "mesh_length = []\n",
    "chemical_length = []\n",
    "\n",
    "for feature in features.keys():\n",
    "    for _, meta in metadata_dict.items():\n",
    "        if len(meta[feature]) > 0:\n",
    "            features[feature] += 1\n",
    "            if feature == 'mesh':\n",
    "                mesh_length.append(len(meta[feature]))\n",
    "            \n",
    "            elif feature == 'chemical':\n",
    "                chemical_length.append(len(meta[feature]))\n",
    "                \n",
    "            \n",
    "print(\"Ocurred features \", features)\n",
    "print(\"avg mesh length \", np.mean(mesh_length))\n",
    "print(\"avg chemical length \", np.mean(chemical_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0c402",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('first_cluster_metadata.json', 'w') as json_file:\n",
    "    json.dump(metadata_dict, json_file, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b161bb",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the metadata file\n",
    "with open('first_cluster_metadata.json', 'r') as json_file:\n",
    "    metadata_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9512e5",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_dict_similarity(dict_1, dict_2, mode ): \n",
    "    similarity = 0\n",
    "    modes = ['number_of_common_terms', 'jaccard']\n",
    "    \n",
    "    assert mode in modes\n",
    "    \n",
    "    for key in dict_1:\n",
    "            if key in dict_2:\n",
    "                similarity += 1\n",
    "                \n",
    "    \n",
    "    if mode == 'number_of_common_terms':\n",
    "        pass\n",
    "       \n",
    "    elif mode == 'jaccard':\n",
    "        similarity = similarity / (len(dict_1) + len(dict_2))\n",
    "        \n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7e782",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate similarity between two pmid's\n",
    "#Lets call it metric for now. This is unnormalized\n",
    "def calculate_similarity(pmid1, pmid2, feature):\n",
    "    \n",
    "    metric = 0\n",
    "    pmid1 = str(pmid1)\n",
    "    pmid2 = str(pmid2)\n",
    "    \n",
    "    assert feature in ['year', 'mesh', 'chemical', 'co-citation', 'bib-coupling']\n",
    "    if feature == 'year':             \n",
    "                year1 = metadata_dict[pmid1][feature]\n",
    "                year2 = metadata_dict[pmid2][feature]\n",
    "                \n",
    "                if len(year1) > 0 and len(year2)>0:\n",
    "                    metric = np.abs(int(year1)-int(year2))\n",
    "                \n",
    "                else:\n",
    "                    metric = -1\n",
    "    \n",
    "    \n",
    "    #They are both treated the same way ...\n",
    "    elif feature == 'mesh' or feature == 'chemical':\n",
    "        common_terms = 0\n",
    "        terms_1 = {}\n",
    "        terms_2 = {}\n",
    "        \n",
    "        for term in metadata_dict[pmid1][feature]:\n",
    "            terms_1[term[0]] = 1\n",
    "    \n",
    "        for term in metadata_dict[pmid2][feature]:\n",
    "            terms_2[term[0]] = 1\n",
    "        \n",
    "        if len(terms_1) > 0 and len(terms_2)>0:\n",
    "            metric = calculate_dict_similarity(terms_1, terms_2, 'jaccard')\n",
    "                    \n",
    "        else:\n",
    "            metric = -1\n",
    "            \n",
    "    elif feature == 'bib-coupling':\n",
    "        node_1 = node_lookup_dict[pmid1]\n",
    "        node_2 = node_lookup_dict[pmid2]\n",
    "        \n",
    "        common = np.dot(adj_matrix[node_1,:], adj_matrix[node_2, :])\n",
    "        denom = np.sum(adj_matrix[node_1, :]) + np.sum(adj_matrix[node_2, :]) - common\n",
    "        \n",
    "        if denom > 0 :\n",
    "            metric = common / denom\n",
    "        \n",
    "    \n",
    "    elif feature == 'co-citation':\n",
    "        node_1 = node_lookup_dict[pmid1]\n",
    "        node_2 = node_lookup_dict[pmid2]\n",
    "        \n",
    "        common = np.dot(adj_matrix[: , node_1], adj_matrix[: , node_2])\n",
    "        denom = np.sum(adj_matrix[:, node_1]) + np.sum(adj_matrix[:, node_2]) - common\n",
    "        \n",
    "        if denom > 0 :\n",
    "            metric = common / denom\n",
    "    return metric\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c7268",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_three_hop_similarity():\n",
    "    #Is currently inefficient as relies on matrix multiplication\n",
    "    # We use undirected edges\n",
    "    aggregated_three_hop_similarity = np.zeros_like(adj_matrix)\n",
    "    undirected_adj_matrix = adj_matrix + adj_matrix.transpose()\n",
    "    second_hop_distance = np.matmul(undirected_adj_matrix, undirected_adj_matrix)\n",
    "    \n",
    "    three_hop_distance = np.matmul(second_hop_distance, undirected_adj_matrix)\n",
    "    \n",
    "    for i in range(adj_matrix.shape[0]):\n",
    "        for j in range(i):\n",
    "            similarity = 0\n",
    "            if undirected_adj_matrix[i][j] == 1:\n",
    "                similarity = 1\n",
    "            \n",
    "            elif second_hop_distance[i][j] > 0:\n",
    "                similarity = 2/3\n",
    "            \n",
    "            elif three_hop_distance[i][j] > 0:\n",
    "                similarity = 1/3\n",
    "            \n",
    "            aggregated_three_hop_similarity[i][j] = similarity\n",
    "    \n",
    "    aggregated_three_hop_similarity = aggregated_three_hop_similarity + aggregated_three_hop_similarity.transpose()\n",
    "    return aggregated_three_hop_similarity\n",
    "\n",
    "aggregated_three_hop_similarity = calculate_three_hop_similarity()\n",
    "np.save('aggregated_three_hop_similarity.npy', aggregated_three_hop_similarity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7028dea",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!ls\n",
    "!rm aggregated_three_hop_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748c102",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all mesh terms are tuples, so we use UI's as they are standard\n",
    "\n",
    "ones = 0\n",
    "twos = 0\n",
    "\n",
    "for key, value in metadata_dict.items():\n",
    "    if len(value['mesh']) >0:\n",
    "        for mesh in value['mesh']:\n",
    "            if len(mesh) == 1:\n",
    "                ones += 1\n",
    "            \n",
    "            elif len(mesh) == 2:\n",
    "                twos += 1\n",
    "            \n",
    "            else:\n",
    "                raise 'error'\n",
    "            \n",
    "print(f'Mesh terms with one entires {ones}')\n",
    "print(f'Mesh terms with two entires {twos}')\n",
    "print(f'Example of a mesh: {mesh}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae28d79",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def report_stats(vals, title):\n",
    "    filtered_val = []\n",
    "    for val in vals:\n",
    "        if val != -1:\n",
    "            filtered_val.append(val)\n",
    "            \n",
    "    median = np.median(filtered_val)\n",
    "    mean = np.mean(filtered_val)\n",
    "    min = np.amin(filtered_val)\n",
    "    max = np.amax(filtered_val)\n",
    "    total = np.sum(filtered_val)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "#     fig.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(vals)\n",
    "    \n",
    "    labels = [title]\n",
    "\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    ax.set_title(f'{title}  boxplot diagram')\n",
    "#     ax.set_xlabel(f'{title}')\n",
    "    ax.set_ylabel('Values')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(f'median {median} mean {mean} min {min} max {max}')\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_similarity_matrix(feature):\n",
    "    similarity_matrix = np.zeros((len(nodes_array), len(nodes_array))) \n",
    "    \n",
    "    for i in tqdm(range(similarity_matrix.shape[0])):\n",
    "        for j in range(i):\n",
    "            pmid1 = str(nodes_array[i])\n",
    "            pmid2 = str(nodes_array[j])            \n",
    "            similarity_matrix[i][j] = calculate_similarity(pmid1, pmid2, feature)\n",
    "        \n",
    "    similarity_matrix = similarity_matrix + similarity_matrix.transpose()\n",
    "    \n",
    "    if feature == 'year':\n",
    "        similarity_matrix = np.ones_like(similarity_matrix) - similarity_matrix/np.amax(similarity_matrix)\n",
    "        similarity_matrix = np.where(similarity_matrix < 0, -1, similarity_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "year_similarity_matrix = calculate_similarity_matrix('year')\n",
    "np.save('year_similarity_matrix.npy', year_distance_matrix)\n",
    "\n",
    "mesh_similarity_matrix = calculate_similarity_matrix('mesh')\n",
    "np.save('mesh_similarity_matrix.npy', mesh_similarity_matrix)\n",
    "\n",
    "\n",
    "bib_coupling_similarity_matrix = calculate_similarity_matrix('bib-coupling')\n",
    "np.save('bib_coupling_similarity_matrix.npy', bib_coupling_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "cocitation_similarity_matrix = calculate_similarity_matrix('co-citation')\n",
    "np.save('cocitation_similarity_matrix.npy', cocitation_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "chemical_similarity_matrix = calculate_similarity_matrix('chemical')\n",
    "np.save('chemical_similarity_matrix.npy', chemical_similarity_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad8626da",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cocitation_matrix = np.load('cocitation_matrix.npy')\n",
    "bib_coupling_matrix = np.load('bib_coupling_matrix.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad3cec",
   "metadata": {
    "is_executing": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Assuming a square matrix, report statistics\n",
    "def report_matrix_stats(matrix, title):\n",
    "    n = matrix.shape[0]\n",
    "    all_values = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            #Remove negatives as they are flags\n",
    "            if matrix[i][j] > -1:\n",
    "                all_values.append(matrix[i][j])\n",
    "    \n",
    "    report_stats(all_values, title)\n",
    "    \n",
    "    \n",
    "report_matrix_stats(year_distance_matrix, 'year')\n",
    "report_matrix_stats(mesh_similarity_matrix, 'jaccard mesh similarity')\n",
    "report_matrix_stats(chemical_similarity_matrix, 'jaccard chemical similarity')\n",
    "report_matrix_stats(cocitation_similarity_matrix, 'jaccard co-citation similarity')\n",
    "report_matrix_stats(bib_coupling_similarity_matrix, 'jaccard bib-coupling similarity')\n",
    "report_matrix_stats(aggregated_three_hop_similarity, 'aggregated three hop similarity')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea75bd3",
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Verify that those entries without grant actually do not have grants\n",
    "sample_list_of_pmid_without_grant = []\n",
    "without_grants_doi = []\n",
    "i = 0\n",
    "with_grants = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if 'grant' in str(value).lower():\n",
    "        with_grants.append(key)\n",
    "    \n",
    "    else:\n",
    "        sample_list_of_pmid_without_grant.append(key)\n",
    "        without_grants_doi.append(doi_lookup_dict[key])\n",
    "        \n",
    "    i+= 1\n",
    "    \n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "print('Statistics for first 10 papers:\\n')\n",
    "print(f'DOIs without grant: {without_grants_doi[0:10]}')\n",
    "print(f'PMIDs without grant: {sample_list_of_pmid_without_grant[0:10]}\\n')\n",
    "\n",
    "print(f'PMID of papers with grants {with_grants}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd746ba",
   "metadata": {},
   "source": [
    "<h1>Example</h1>\n",
    "An example of bib-couple and co-citation calculation for two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7784fe88",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recorded bib-coupling: 6.0\n",
      "crossed checked bib-coupling: 6\n",
      "\n",
      "recorded co-citation: 99.0\n",
      "crossed checked co-citation: 99\n"
     ]
    }
   ],
   "source": [
    "first_node_id = 0\n",
    "first_node_pmid = nodes_array[first_node_id]\n",
    "\n",
    "second_node_id = 50\n",
    "second_node_pmid = nodes_array[second_node_id]\n",
    "\n",
    "first_reference = {}\n",
    "first_cited = {}\n",
    "\n",
    "second_reference = {}\n",
    "second_cited = {}\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if start_node == first_node_pmid:\n",
    "        first_reference[end_node] = 1\n",
    "    if end_node == first_node_pmid:\n",
    "        first_cited[start_node] = 1     \n",
    "        \n",
    "    \n",
    "    if start_node == second_node_pmid:\n",
    "        second_reference[end_node] = 1\n",
    "    if end_node == second_node_pmid:\n",
    "        second_cited[start_node] = 1       \n",
    "\n",
    "print(f'recorded bib-coupling: {bib_coupling_matrix[first_node_id][second_node_id]}')\n",
    "print(f'crossed checked bib-coupling: {calculate_dict_similarity(first_reference, second_reference, mode = \"number_of_common_terms\")}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'recorded co-citation: {cocitation_matrix[first_node_id][second_node_id]}')\n",
    "print(f'crossed checked co-citation: {calculate_dict_similarity(first_cited, second_cited, mode = \"number_of_common_terms\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afec573",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "<h1>Missing ISSN</h1>\n",
    "Finding xml's without ISSN. Any journal info?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59a69e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing ISSN pmids ['3645914', '14066214', '1211972', '10474434', '3429506', '11460150', '4347574', '4713388', '4308132', '4383515', '5119427', '12335001', '5631946', '5293199', '5162804', '6377493', '6513295', '290137', '332795', '234110', '151982', '544612', '62782', '1282098', '1691743', '9877917', '3043083', '3710688', '4233264', '4447963', '4871582', '4506117', '11441495', '12163935', '4789929', '4286257', '5084293', '5302502', '5624558', '5377235', '6568438', '410866', '1797831']\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "missing_issn = []\n",
    "for key, value in all_xmls.items():\n",
    "    if not 'issn' in str(value).lower():\n",
    "        missing_issn.append(key)\n",
    "        \n",
    "print(f'missing ISSN pmids {missing_issn}')\n",
    "print(len(missing_issn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
