{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e291e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T18:08:00.933004153Z",
     "start_time": "2023-12-18T18:08:00.518079580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Postgres successfully\n",
      "(11233, 11233)\n",
      "(26264, 26264)\n",
      "(24416, 24416)\n",
      "(4790, 4790)\n",
      "(26745, 26745)\n",
      "(18803, 18803)\n",
      "(12502, 12502)\n",
      "(5468, 5468)\n",
      "(29089, 29089)\n",
      "(4326, 4326)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the PGDATABASE environment variable\n",
    "os.environ[\"PGDATABASE\"] = \"ernieplus\"\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\"\")\n",
    "print(\"Connected to Postgres successfully\")\n",
    "# conn.close()\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Define your SQL query\n",
    "sql_query = \"select * from hm31.aligned_musicians am limit 10\"\n",
    "\n",
    "# Execute the SQL query\n",
    "cur.execute(sql_query)\n",
    "\n",
    "# Fetch and print the result (you can modify this part based on your query)\n",
    "result = cur.fetchall()\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f303592",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:08:00.968193021Z",
     "start_time": "2023-12-18T18:08:00.553163851Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/first_cluster_edges.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-62b2d7dc015b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mmultiprocessing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mProcess\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mManager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPool\u001B[0m  \u001B[0;31m# Import the Pool class\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mnodes_array\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0medge_array\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0massert_edges_are_within_first_cluster\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mnode_lookup_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/step_1/step_1_sample_study/load_data.py\u001B[0m in \u001B[0;36massert_edges_are_within_first_cluster\u001B[0;34m()\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0massert_edges_are_within_first_cluster\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 93\u001B[0;31m     \u001B[0medges\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen_csv_first_cluster_nodes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../data/first_cluster_edges.tsv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     94\u001B[0m     \u001B[0mnodes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen_csv_first_cluster_nodes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../data/first_cluster_nodes.tsv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/step_1/step_1_sample_study/load_data.py\u001B[0m in \u001B[0;36mopen_csv_first_cluster_nodes\u001B[0;34m(file)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mopen_csv_first_cluster_nodes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'reformatted.tsv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnewline\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcsv_file\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m         \u001B[0mcsv_reader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcsv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcsv_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/first_cluster_edges.tsv'"
     ]
    }
   ],
   "source": [
    "#Load edges and nodes\n",
    "\n",
    "import numpy as np\n",
    "from load_data import *\n",
    "from multiprocessing import Process, Manager, Pool  # Import the Pool class\n",
    "\n",
    "nodes_array, edge_array = assert_edges_are_within_first_cluster()\n",
    "\n",
    "node_lookup_dict = {}\n",
    "\n",
    "min_index = np.amin(nodes_array)\n",
    "max_index = np.amax(nodes_array)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for node in nodes_array:\n",
    "    node_lookup_dict[str(node)] = i\n",
    "    i+= 1\n",
    "    \n",
    "adj_matrix = np.load('adj_matrix.npy')    \n",
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeaad6",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.567864330Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Get dois from the exosome csv\n",
    "# #Read the DOIS from the node id\n",
    "\n",
    "# from load_data import *\n",
    "\n",
    "# import csv\n",
    "\n",
    "# csv_file_path = data_dir + 'exosome.csv'\n",
    "# doi_lookup_dict = {}\n",
    "\n",
    "# # Open the CSV file for reading\n",
    "# with open(csv_file_path, 'r') as csv_file:\n",
    "#     csv_reader = csv.reader(csv_file)\n",
    "#     dois = {}\n",
    "#     for line_number, row in tqdm(enumerate(csv_reader), total = 14695476):  # 'total' is the total number of iterations\n",
    "        \n",
    "#         if line_number <min_index or line_number > max_index:\n",
    "#             continue\n",
    "            \n",
    "#         id = str(row[0])\n",
    "#         doi = row[2]\n",
    "        \n",
    "#         if id in node_lookup_dict:\n",
    "#             doi_lookup_dict[id] = doi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c810cf",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.572997839Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save\n",
    "# import json\n",
    "\n",
    "# with open('first_cluster_dois.json', 'w') as json_file:\n",
    "#     json.dump(doi_lookup_dict, json_file, indent=4)\n",
    "    \n",
    "# with open('first_cluster_dois.json', 'w') as json_file:\n",
    "#     doi_lookup_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f66d16",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.573109028Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get PMID by doi\n",
    "\n",
    "import requests\n",
    "import xmltodict\n",
    "\n",
    "doi_dict = {}\n",
    "\n",
    "#Get dois\n",
    "def fetch_pmid_from_doi(doi='10.1073/pnas.0510928103'):\n",
    "\n",
    "    pmid_dict = {}\n",
    "    request_str = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids='\n",
    "    request_str += str(doi)\n",
    "    \n",
    "    response = requests.get(request_str)\n",
    "    response_text = response.text\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        index = response_text.index('pmid=')\n",
    "\n",
    "\n",
    "        pmid_whole = response_text[index:].split(' ')[0]\n",
    "        pmid = int(pmid_whole[:-1].split('\\\"')[1])\n",
    "\n",
    "        return pmid\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# for key in tqdm(node_lookup_dict.keys()):\n",
    "#     doi_dict[key] = fetch_pmid_from_doi(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe91773",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.575159196Z"
    }
   },
   "outputs": [],
   "source": [
    "node_id_to_PMID_mapping = {}\n",
    "\n",
    "with open('first_cluster_pmids.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for line_number, row in tqdm(enumerate(csv_reader)):  # 'total' is the total number of iterations\n",
    "        \n",
    "        if line_number == 0:\n",
    "            continue\n",
    "            \n",
    "        node_id,cluster_id, doi, pmid = row\n",
    "        node_id_to_PMID_mapping[node_id] = pmid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e21bf",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.577399901Z"
    }
   },
   "outputs": [],
   "source": [
    "#It turns out that initial node ids is their PMID\n",
    "\n",
    "\n",
    "# for key, value in doi_dict.items():\n",
    "#     assert int(key) == int(value)    \n",
    "\n",
    "# with open('first_cluster_pmid.json', 'w') as json_file:\n",
    "#     json.dump(doi_dict, json_file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce168bb",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.620942778Z"
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('first_cluster_pmid.json', 'r') as json_file:\n",
    "#     doi_dict = json.load(json_file)\n",
    "    \n",
    "# pmid_dict = doi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2af2d",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621070899Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save all xmls\n",
    "all_xmls = {}\n",
    "import time\n",
    "\n",
    "def save_all_xmls(pmid):\n",
    "    wait = 0.25\n",
    "    time.sleep(wait)\n",
    "    pmid_dict = {}\n",
    "    request_str  = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id='\n",
    "    request_str += str(pmid)\n",
    "    \n",
    "    response = requests.get(request_str)\n",
    "    response_text = response.text\n",
    "    \n",
    "    try:\n",
    "        xml_dict = xmltodict.parse(response_text)\n",
    "        \n",
    "    except:\n",
    "        time.sleep(2 * wait)\n",
    "        return fetch_metadata_from_pmid(pmid)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "# for node_id, pmid in tqdm(node_id_to_PMID_mapping.items()):\n",
    "#     all_xmls[node_id] = save_all_xmls(pmid)\n",
    "\n",
    "# with open('first_cluster_xmls.json', 'w') as json_file:\n",
    "#     json.dump(all_xmls, json_file, indent=4)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9485977d",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621210061Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load all XMLs\n",
    "import json\n",
    "\n",
    "with open('first_cluster_xmls.json', 'r') as json_file:\n",
    "    all_xmls = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db269bba",
   "metadata": {},
   "source": [
    "<h1>Code to extract metadata from saved XMLs by parsing them </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29decb9",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621293738Z"
    }
   },
   "outputs": [],
   "source": [
    "#Mispelled\n",
    "import time\n",
    "import requests\n",
    "import xmltodict\n",
    "import tqdm\n",
    "\n",
    "parsed_error = []\n",
    "dict_of_interest = {'title':0, 'abstract': 0, 'keyword': 0, \"grantlist\": 0, \"meshheadinglist\": 0, \"chemicallist\":0, 'datecompleted': 0, 'journal': 0}\n",
    "journal_title_lookup_dict = {}\n",
    "journal_ISSN_lookup_dict = {}\n",
    "\n",
    "def fetch_metadata_from_pmid(response_text, idx, pmid): \n",
    "    global dict_of_interest\n",
    "    mesh_headings = []\n",
    "    grants = []\n",
    "    year = \"\"\n",
    "    journal_ISSN = \"\"\n",
    "    abstract = \"\"\n",
    "    chemical_list = []\n",
    "    meta_data = {}\n",
    "    journal_title = \"\"\n",
    "    pub_year = \"\"\n",
    "    keyword_list = []\n",
    "    article_title = \"\"\n",
    "\n",
    "    try:\n",
    "        xml_dict = xmltodict.parse(response_text)\n",
    "        if len(str(xml_dict))< 300:\n",
    "            parsed_error.append(pmid)\n",
    "            raise 'no data'\n",
    "        \n",
    "        for key_of_interest in dict_of_interest.keys():\n",
    "            if key_of_interest in str(xml_dict).lower():\n",
    "\n",
    "                dict_of_interest[key_of_interest] += 1\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if 'DateCompleted' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "                if 'Year' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['DateCompleted']:\n",
    "                    year = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['DateCompleted']['Year']\n",
    "\n",
    "            else:\n",
    "                  pass\n",
    "    \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "               if 'Abstract' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article'].keys():\n",
    "                    if 'AbstractText' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Abstract']:\n",
    "                        abstract = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Abstract']['AbstractText']\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "               if 'ArticleTitle' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article'].keys():\n",
    "                    article_title = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['ArticleTitle']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            if 'JournalIssue' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                if 'PubDate' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']:\n",
    "                    if 'Year' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']:\n",
    "                        pub_year = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "                        \n",
    "            if 'ISSN' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                journal_ISSN = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['ISSN']['#text'] \n",
    "            \n",
    "            if 'Title' in  xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "                journal_title = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['Title']\n",
    "                \n",
    "            if len(journal_ISSN) > 0 and len(journal_title)>0:\n",
    "                journal_ISSN_lookup_dict[journal_ISSN] = journal_title\n",
    "                journal_title_lookup_dict[journal_title] = journal_ISSN\n",
    "                \n",
    "            elif len(journal_ISSN) == 0:\n",
    "                if journal_title in journal_title_lookup_dict:\n",
    "                    journal_ISSN = journal_title_lookup_dict[journal_title]\n",
    "                    \n",
    "            elif len(journal_title) == 0:\n",
    "                if journal_ISSN in journal_ISSN_lookup_dict:\n",
    "                    journal_title = journal_ISSN_lookup_dict[journal_ISSN]                \n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e, pmid)\n",
    "        \n",
    "        \n",
    "        #Grant\n",
    "        #Very few grants don't have grant id's\n",
    "        #Grant institute could also be relevant\n",
    "        #TODO: collect missing grant id\n",
    "\n",
    "        try:\n",
    "            if 'GrantList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']:\n",
    "                if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']) == list:\n",
    "                    for grant in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']:\n",
    "                        if 'GrantID' in grant:\n",
    "                            grants.append((grant['GrantID']))\n",
    "                            \n",
    "                else:\n",
    "                     grants.append(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant']['GrantID'])\n",
    "                     pass   \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['GrantList']['Grant'][0])\n",
    "        \n",
    "        #MeSH heading  \n",
    "        \n",
    "        try:\n",
    "              if 'KeywordList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "#                     print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList'].keys())\n",
    "#                     print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword'])\n",
    "#                     print(type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']))\n",
    "#                     print(type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']))\n",
    "                    if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']) == list:\n",
    "                        for key_word in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']:\n",
    "                            keyword_list.append(key_word['#text'])\n",
    "                    \n",
    "                    else:\n",
    "                        keyword_list.append(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['KeywordList']['Keyword']['#text'])\n",
    "                    \n",
    "\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']))\n",
    "        \n",
    "            #DescriptorName? Is a term\n",
    "            if 'MeshHeadingList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "                if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']) == list:\n",
    "                    for mesh in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']:\n",
    "    #                 print(mesh['DescriptorName'])\n",
    "    #                 print(mesh['DescriptorName']['@UI'])\n",
    "    #                 print(mesh['DescriptorName']['#text'])\n",
    "    \n",
    "                        if '@Type' in mesh['DescriptorName'].keys() and mesh['DescriptorName']['@Type'] == 'Geographic':\n",
    "                            continue\n",
    "                \n",
    "                \n",
    "                        mesh_headings.append((mesh['DescriptorName']['@UI']))\n",
    "            \n",
    "                else:\n",
    "                    mesh = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']['DescriptorName']\n",
    "                    if  (not '@Type' in mesh.keys() or mesh['@Type'] != 'Geographic'):\n",
    "                        mesh_headings.append(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']['DescriptorName']['@UI'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        try:\n",
    "        #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['MeshHeadingList']['MeshHeading']))\n",
    "        \n",
    "            #DescriptorName? Is a term\n",
    "            if 'ChemicalList' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']:\n",
    "#                print(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']\n",
    "# #                      ['ChemicalList']['Chemical']['NameOfSubstance'])\n",
    "               #print(len(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']))\n",
    "               if type(xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']) == list:\n",
    "                   for substance in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']:\n",
    "                        if substance['NameOfSubstance']['@UI'][0].lower() == 'c':\n",
    "                            chemical_list.append(substance['NameOfSubstance']['@UI'])\n",
    "                        \n",
    "               else:\n",
    "                       if substance['NameOfSubstance']['@UI'][0].lower() == 'c':\n",
    "                           xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['ChemicalList']['Chemical']['NameOfSubstance']['@UI'][0]\n",
    "                            \n",
    "                #print(chemical_list)\n",
    "                \n",
    "               \n",
    "              \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "        #References and history\n",
    "        #print(xml_dict['PubmedArticleSet']['PubmedArticle']['PubmedData'].keys())\n",
    "\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    if len(year) == 0:\n",
    "        year = pub_year\n",
    "        \n",
    "    meta_data = {'mesh': mesh_headings, 'grants': grants, 'year': year, \n",
    "                 'journal_ISSN': journal_ISSN,'journal_title': journal_title,\n",
    "                 'chemical' : chemical_list, 'pub_year': pub_year, 'keyword': keyword_list,\n",
    "                 'author': {}, 'title': article_title, 'abstract': abstract} \n",
    "    \n",
    "#     meta_data = {'title': article_title, 'abstract' : abstract}\n",
    "        \n",
    "    return meta_data\n",
    "\n",
    "metadata_dict = {}\n",
    "\n",
    "# [166, 719, 1672, 1918] odd\n",
    "\n",
    "idx = 0\n",
    "for key, value in tqdm.tqdm(all_xmls.items()):\n",
    "    metadata_dict[key] = fetch_metadata_from_pmid(value, idx, key)\n",
    "    idx += 1\n",
    "    \n",
    "\n",
    "\n",
    "print(f'failed parse {len(parsed_error)} which are: {parsed_error}')\n",
    "print(f'Occured keyword dict: {dict_of_interest}')\n",
    "\n",
    "# with open('first_cluster_metadata.json', 'w') as json_file:\n",
    "#     json.dump(metadata_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d5c04",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621362487Z"
    }
   },
   "outputs": [],
   "source": [
    "def access_metadata_with_node_id(node_id):\n",
    "    return metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e5d1b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621417691Z"
    }
   },
   "outputs": [],
   "source": [
    "# microrna_title = 0\n",
    "# microrna_abstract = 0\n",
    "\n",
    "# for key, value in metadata_dict.items():\n",
    "#     if 'microrna' in value['title'].lower():\n",
    "#         microrna_title += 1\n",
    "#     print(type(value['abstract']))\n",
    "    \n",
    "#     if type(value['abstract']) == str:    \n",
    "    \n",
    "#         microrna_abstract += 1\n",
    "      \n",
    "# print(microrna_title)\n",
    "# print(microrna_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018691bc",
   "metadata": {},
   "source": [
    "<h1>Adding author information </h1>\n",
    "<p> Now through Scopus API (to which we have subscription) we retrieve authors for each dois. Turns out 2003/2004 papers have doi, and some authors are not available</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ef219",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621682460Z"
    }
   },
   "outputs": [],
   "source": [
    "#Wait 1 minute for this cell to run ...\n",
    "from pybliometrics.scopus import AbstractRetrieval\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "pmid_authors_dict = {}\n",
    "i = 0\n",
    "failed_authors = []\n",
    "for node_id, pmid in tqdm(node_id_to_PMID_mapping.items()):\n",
    "\n",
    "    author_id_array = []\n",
    "\n",
    "    try:\n",
    "        ab = AbstractRetrieval(pmid)\n",
    "        for author in ab.authors:\n",
    "            author_id_array.append(author.auid)      \n",
    "    except:\n",
    "        failed_authors.append(pmid)\n",
    "        \n",
    "    pmid_authors_dict[pmid] = author_id_array\n",
    "\n",
    "err = 0\n",
    "for pmid, auth_arr in pmid_authors_dict.items():\n",
    "    metadata_dict[pmid]['author'] = auth_arr\n",
    "\n",
    "# with open('meta_data_dict.json', 'w') as json_file:\n",
    "#     json.dump(metadata_dict, json_file, indent=4)        \n",
    " \n",
    "print(f'failed: {len(failed_authors)} success: {len(pmid_authors_dict)-len(failed_authors)} err {err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb6921",
   "metadata": {},
   "source": [
    "Turns out that out of 2004 papers of CEN cluster1, 11 of those don't have any metadata. That leaves us with 1993 samples. The above dictionary tells us the frequence of keyword occurence. The bellow one tells us the frequency of actual recorded value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2472e",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621755828Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Count incomplete data\n",
    "\n",
    "features = {'keyword':0, 'mesh': 0, 'grants': 0, 'year': 0,\n",
    "             \"journal_ISSN\":0, 'chemical' : 0, 'pub_year': 0}\n",
    "#Mesh terms are sometimes unrelated\n",
    "\n",
    "mesh_length = []\n",
    "chemical_length = []\n",
    "\n",
    "for feature in features.keys():\n",
    "    for _, meta in metadata_dict.items():\n",
    "        if len(meta[feature]) > 0:\n",
    "            features[feature] += 1\n",
    "            if feature == 'mesh':\n",
    "                mesh_length.append(len(meta[feature]))\n",
    "            \n",
    "            elif feature == 'chemical':\n",
    "                chemical_length.append(len(meta[feature]))\n",
    "                \n",
    "            \n",
    "print(\"recorded features \", features)\n",
    "print(f\"avg mesh length: {np.mean(mesh_length)} std: {np.std(mesh_length)}\" )\n",
    "print(f\"avg chemical length: {np.mean(chemical_length)} std: {np.std(chemical_length)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15eba5b",
   "metadata": {},
   "source": [
    "<h2> Now stripping nodes to the ones that have dois </h2>\n",
    "<p> Only a subset of 2004 nodes have dois. We will extract the relevant adjacency matrix </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691eadfb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621809689Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(metadata_dict.keys())\n",
    "# print(node_id_to_PMID_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affeb5f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621861326Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "new_node_array = []\n",
    "selected_indices = []\n",
    "\n",
    "for idx, node in tqdm(enumerate(nodes_array)):\n",
    "    if str(node) in node_id_to_PMID_mapping:\n",
    "        pmid = node_id_to_PMID_mapping[str(node)]\n",
    "        assert str(pmid) in metadata_dict\n",
    "        new_node_array.append(int(node))\n",
    "        selected_indices.append(idx)\n",
    "        \n",
    "new_adjacency_matrix = adj_matrix[selected_indices][:, selected_indices]\n",
    "print(new_adjacency_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837c969",
   "metadata": {},
   "source": [
    "<h2> George's file </h2>\n",
    "<p> a dataframe of doi, in deg, out deg, pmid, title, abstract boolean. new_noe_array and adj_matrix are now only for nodes with doi</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61e1db",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621913845Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "PMID_to_node_id_mapping = {}\n",
    "george_df = []\n",
    "\n",
    "for node_id, pmid in node_id_to_PMID_mapping.items():\n",
    "    PMID_to_node_id_mapping[pmid] = node_id\n",
    "\n",
    "for pmid, node_id in PMID_to_node_id_mapping.items():\n",
    "    idx_in_adj_matrix = new_node_array.index(int(node_id))\n",
    "    in_degree = np.sum(new_adjacency_matrix[:, idx_in_adj_matrix])\n",
    "    out_degree = np.sum(new_adjacency_matrix[idx_in_adj_matrix, :])\n",
    "    title = metadata_dict[str(pmid)]['title']\n",
    "    abstract_presence = len(metadata_dict[str(pmid)]['abstract']) > 0\n",
    "    \n",
    "    george_df.append({'pmid': pmid, 'in_degree': in_degree, 'out_degree':out_degree,\n",
    "                      'title':title, 'has_abstract': abstract_presence})\n",
    "    \n",
    "# george_df = pd.DataFrame(george_df)\n",
    "# print(george_df.head())\n",
    "\n",
    "# adj_matrix = new_adjacency_matrix\n",
    "# nodes_array = new_node_array\n",
    "\n",
    "# george_df.to_csv('georges_csv_for_manual_evaluation.csv', index=False) \n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b161bb",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.621967335Z"
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Load the metadata file\n",
    "# with open('meta_data_dict.json', 'r') as json_file:\n",
    "#     metadata_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9512e5",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622023441Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_dict_similarity(dict_1, dict_2, mode ): \n",
    "    similarity = 0\n",
    "    modes = ['number_of_common_terms', 'jaccard']\n",
    "    \n",
    "    assert mode in modes\n",
    "    \n",
    "    for key in dict_1:\n",
    "            if key in dict_2:\n",
    "                similarity += 1\n",
    "                \n",
    "    \n",
    "    if mode == 'number_of_common_terms':\n",
    "        pass\n",
    "       \n",
    "    elif mode == 'jaccard':\n",
    "        similarity = similarity / (len(dict_1) + len(dict_2))\n",
    "        \n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443af2b5",
   "metadata": {},
   "source": [
    "<h1> Defining MeSH similarity </h1>\n",
    "<p> First we need to find similarity for each MeSH pair. For now we just find the number of common terms by adding a . after the first letter. We will normalize by the total number.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67e175",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622080548Z"
    }
   },
   "outputs": [],
   "source": [
    "max_mesh_overlap = 0\n",
    "import json\n",
    "\n",
    "with open('all_mesh_terms.json', 'r') as json_file:\n",
    "    all_mesh_terms = json.load(json_file)\n",
    "    \n",
    "# print(all_mesh_terms)\n",
    "    \n",
    "def calculate_mesh_pair_similarity(mesh1, mesh2):\n",
    "    global max_mesh_overlap\n",
    "\n",
    "    mesh1 = mesh1[0] + '.' + mesh1[1:]\n",
    "    mesh2 = mesh2[0] + '.' + mesh2[1:]    \n",
    "    \n",
    "    mesh_1_decomp = mesh1.split('.')\n",
    "    mesh_2_decomp = mesh2.split('.')\n",
    "    \n",
    "    \n",
    "    common_terms = 0\n",
    "    limit = min(len(mesh_1_decomp), len(mesh_2_decomp))\n",
    "    for i in range(limit):\n",
    "        if mesh_1_decomp[i] == mesh_2_decomp[i]:\n",
    "            common_terms += 1\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    if common_terms > max_mesh_overlap:\n",
    "        max_mesh_overlap = common_terms\n",
    "        \n",
    "    return common_terms\n",
    "\n",
    "print(calculate_mesh_pair_similarity('C23.300.820', 'C23.550.291.125'))\n",
    "\n",
    "def calculate_mesh_similarity_for_two_papers(pmid1, pmid2, mode):\n",
    "    assert mode in ['median', 'mean', 'max', 'min']\n",
    "    similarities = []\n",
    "    mesh_terms_1 = metadata_dict[pmid1]['mesh']\n",
    "    mesh_terms_2 = metadata_dict[pmid2]['mesh']\n",
    "    \n",
    "    for first in mesh_terms_1:\n",
    "        first_mesh_tree = all_mesh_terms[first]\n",
    "        for second in mesh_terms_2:\n",
    "            second_mesh_tree = all_mesh_terms[second]\n",
    "            similarities.append(calculate_mesh_pair_similarity(first_mesh_tree, second_mesh_tree))\n",
    "    \n",
    "    if len(similarities) == 0:\n",
    "        return -1\n",
    "    \n",
    "    else:\n",
    "        if mode == 'median':\n",
    "            return np.median(similarities)\n",
    "        \n",
    "        if mode == 'mean':\n",
    "            return np.mean(similarities)\n",
    "        \n",
    "        if mode == 'max':\n",
    "            return np.max(similarities)\n",
    "        \n",
    "        if mode == 'min':\n",
    "            return np.min(similarities)\n",
    "\n",
    "first_dummy_pmid = '6494891'\n",
    "second_dummy_pmid = '15372041'\n",
    "\n",
    "print(calculate_mesh_similarity_for_two_papers(first_dummy_pmid, second_dummy_pmid, 'mean'))   \n",
    "print(metadata_dict[first_dummy_pmid]['mesh'], metadata_dict[second_dummy_pmid]['mesh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7e782",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622131444Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Calculate similarity between two pmid's\n",
    "#Lets call it metric for now. This is unnormalized\n",
    "def calculate_similarity(node_id1, node_id2, feature):\n",
    "    \n",
    "    \n",
    "    metric = 0\n",
    "  \n",
    "    \n",
    "    assert feature in ['keyword','year', 'mesh_jaccard', 'mesh_tree'\n",
    "                       , 'chemical', 'co-citation', 'bib-coupling', 'grants', 'journal', 'author']\n",
    "    if feature == 'year':             \n",
    "                pmid1 = str(node_id_to_PMID_mapping[node_id1])\n",
    "                pmid2 = str(node_id_to_PMID_mapping[node_id2])\n",
    "                year1 = metadata_dict[pmid1][feature]\n",
    "                year2 = metadata_dict[pmid2][feature]\n",
    "                \n",
    "                if len(year1) > 0 and len(year2)>0:\n",
    "                    metric = np.abs(int(year1)-int(year2))\n",
    "                \n",
    "                else:\n",
    "                    metric = -1\n",
    "                    \n",
    "    if feature == 'journal':\n",
    "        \n",
    "        pmid1 = str(node_id_to_PMID_mapping[node_id1])\n",
    "        pmid2 = str(node_id_to_PMID_mapping[node_id2])\n",
    "        \n",
    "        issn1 = metadata_dict[pmid1]['journal_ISSN']\n",
    "        issn2 = metadata_dict[pmid2]['journal_ISSN']\n",
    "        \n",
    "        title1 = metadata_dict[pmid1]['journal_title']\n",
    "        title2 = metadata_dict[pmid2]['journal_title']\n",
    "        \n",
    "        \n",
    "        if len(issn1)>0 and len(issn2)>0:\n",
    "            metric = int(issn1==issn2)\n",
    "        \n",
    "        \n",
    "  \n",
    "        elif len(title1)>0 and len(title2)>0:\n",
    "            metric = int(title1==title2)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            metric = -1\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    #They are both treated the same way ...\n",
    "    elif feature == 'mesh_jaccard' or feature == 'chemical':\n",
    "        \n",
    "        pmid1 = str(node_id_to_PMID_mapping[node_id1])\n",
    "        pmid2 = str(node_id_to_PMID_mapping[node_id2])\n",
    "        \n",
    "        if feature == 'mesh_jaccard':\n",
    "            feature = 'mesh'\n",
    "            \n",
    "        terms_1 = {}\n",
    "        terms_2 = {}\n",
    "        \n",
    "        for term in metadata_dict[pmid1][feature]:\n",
    "            terms_1[term] = 1\n",
    "    \n",
    "        for term in metadata_dict[pmid2][feature]:\n",
    "            terms_2[term] = 1\n",
    "        \n",
    "        if len(terms_1) > 0 and len(terms_2)>0:\n",
    "            metric = calculate_dict_similarity(terms_1, terms_2, 'jaccard')\n",
    "                    \n",
    "        else:\n",
    "            metric = -1\n",
    "            \n",
    "    elif feature == 'mesh_tree':\n",
    "        \n",
    "        pmid1 = str(node_id_to_PMID_mapping[node_id1])\n",
    "        pmid2 = str(node_id_to_PMID_mapping[node_id2])\n",
    "        \n",
    "        metric = calculate_mesh_similarity_for_two_papers(pmid1, pmid2, 'mean')\n",
    "        \n",
    "            \n",
    "    #A slight improvement may be made by passing node_index to this function and skipping indexing altogether\n",
    "    elif feature == 'bib-coupling':\n",
    "        pmid1 = int(node_id1)\n",
    "        pmid2 = int(node_id2)\n",
    "        \n",
    "        \n",
    "        node_1 = nodes_array.index(pmid1)\n",
    "        node_2 = nodes_array.index(pmid2)\n",
    "\n",
    "        \n",
    "        common = np.dot(adj_matrix[node_1,:], adj_matrix[node_2, :])\n",
    "        denom = np.sum(adj_matrix[node_1, :]) + np.sum(adj_matrix[node_2, :]) - common\n",
    "        \n",
    "        if denom > 0 :\n",
    "            metric = (common / denom)\n",
    "        \n",
    "    \n",
    "    elif feature == 'co-citation':\n",
    "        pmid1 = int(node_id1)\n",
    "        pmid2 = int(node_id2)\n",
    "        \n",
    "        \n",
    "        node_1 = nodes_array.index(pmid1)\n",
    "        node_2 = nodes_array.index(pmid2)\n",
    "\n",
    "        common = np.dot(adj_matrix[: , node_1], adj_matrix[: , node_2])\n",
    "        denom = np.sum(adj_matrix[:, node_1]) + np.sum(adj_matrix[:, node_2]) - common\n",
    "        \n",
    "        if denom > 0 :\n",
    "            metric = (common / denom)\n",
    "            \n",
    "    \n",
    "    elif feature == 'grants':\n",
    "        \n",
    "        pmid1 = str(node_id_to_PMID_mapping[node_id1])\n",
    "        pmid2 = str(node_id_to_PMID_mapping[node_id2])\n",
    "        \n",
    "        grants_1 = metadata_dict[pmid1]['grants']\n",
    "        grants_2 = metadata_dict[pmid2]['grants']\n",
    "       \n",
    "        \n",
    "        for first in grants_1:\n",
    "            for second in grants_2:\n",
    "                if first == second:\n",
    "                    metric +=1\n",
    "                    \n",
    "    elif feature == 'author':\n",
    "        \n",
    "        pmid1 = str(node_id_to_PMID_mapping[node_id1])\n",
    "        pmid2 = str(node_id_to_PMID_mapping[node_id2])\n",
    "        \n",
    "        author_1 = {}\n",
    "        author_2 = {}\n",
    "        \n",
    "        for term in metadata_dict[pmid1][feature]:\n",
    "            author_1[term] = 1\n",
    "    \n",
    "        for term in metadata_dict[pmid2][feature]:\n",
    "            author_2[term] = 1\n",
    "        \n",
    "        if len(author_1) > 0 and len(author_2)>0:\n",
    "            metric = calculate_dict_similarity(author_1, author_2, 'jaccard')\n",
    "            \n",
    "        else:\n",
    "            metric = -1\n",
    "       \n",
    "        \n",
    "                    \n",
    "    elif feature == 'keyword':\n",
    "        \n",
    "        pmid1 = str(node_id_to_PMID_mapping[node_id1])\n",
    "        pmid2 = str(node_id_to_PMID_mapping[node_id2])\n",
    "        \n",
    "        keyword_1 = metadata_dict[pmid1]['keyword']\n",
    "        keyword_2 = metadata_dict[pmid2]['keyword']\n",
    "       \n",
    "        if len(keyword_1) == 0 or len(keyword_2)== 0:\n",
    "            metric = -1\n",
    "        \n",
    "        else:\n",
    "            common = 0\n",
    "            for first in keyword_1:\n",
    "                for second in keyword_2:\n",
    "                    if first == second:\n",
    "                        common +=1\n",
    "\n",
    "\n",
    "            metric = common/(len(keyword_1)+len(keyword_2)-common)\n",
    "\n",
    "                    \n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c1570",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622193060Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(new_node_array))\n",
    "save_arr = []\n",
    "import pickle\n",
    "\n",
    "for new_node in nodes_array:\n",
    "    save_arr.append(node_id_to_PMID_mapping[str(new_node)])\n",
    "    \n",
    "file_path = 'first_cluster_pmids_array.pkl'\n",
    "\n",
    "# Dump the array to a file\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(save_arr, file)\n",
    "    \n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c7268",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622249867Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_three_hop_similarity():\n",
    "    #Is currently inefficient as relies on matrix multiplication\n",
    "    # We use undirected edges\n",
    "    aggregated_three_hop_similarity = np.zeros_like(adj_matrix)\n",
    "    undirected_adj_matrix = adj_matrix + adj_matrix.transpose()\n",
    "    second_hop_distance = np.matmul(undirected_adj_matrix, undirected_adj_matrix)\n",
    "    \n",
    "    three_hop_distance = np.matmul(second_hop_distance, undirected_adj_matrix)\n",
    "    \n",
    "    for i in tqdm(range(adj_matrix.shape[0])):\n",
    "        for j in range(i):\n",
    "            similarity = 0\n",
    "            if undirected_adj_matrix[i][j] == 1:\n",
    "                similarity = 0.7\n",
    "            \n",
    "            elif second_hop_distance[i][j] > 0:\n",
    "                similarity = 0.2\n",
    "            \n",
    "            elif three_hop_distance[i][j] > 0:\n",
    "                similarity = 0.1\n",
    "            \n",
    "            aggregated_three_hop_similarity[i][j] = similarity\n",
    "    \n",
    "    aggregated_three_hop_similarity = aggregated_three_hop_similarity + aggregated_three_hop_similarity.transpose()\n",
    "    return aggregated_three_hop_similarity\n",
    "\n",
    "aggregated_three_hop_similarity = calculate_three_hop_similarity()\n",
    "np.save('aggregated_three_hop_similarity.npy', aggregated_three_hop_similarity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95f055",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622302035Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "def report_stats(vals, title, caption, y_axis):\n",
    "    filtered_val = []\n",
    "    for val in vals:\n",
    "        if val >= 0:\n",
    "            filtered_val.append(val)\n",
    "            \n",
    "    median = np.median(filtered_val)\n",
    "    mean = np.mean(filtered_val)\n",
    "    min = np.amin(filtered_val)\n",
    "    max = np.amax(filtered_val)\n",
    "    total = np.sum(filtered_val)\n",
    "    \n",
    "    font = {'family' : 'Times New Roman',\n",
    "#         'weight' : 'bold',\n",
    "        'size'   : 18}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "#     fig.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.boxplot(vals)\n",
    "    \n",
    "    labels = [title]\n",
    "\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "    ax.set_title(f'{title}')\n",
    "    ax.set_xlabel(f'{caption}')\n",
    "    ax.set_ylabel(f'{y_axis}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(f'median {median} mean {mean} min {min} max {max} total {total}')\n",
    "\n",
    "    \n",
    "#Assuming a square matrix, report statistics\n",
    "def report_matrix_stats(matrix, title, caption, y_axis):\n",
    "    n = matrix.shape[0]\n",
    "    all_values = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            #Remove negatives as they are flags\n",
    "            if matrix[i][j] > -1:\n",
    "                all_values.append(matrix[i][j])\n",
    "    \n",
    "    report_stats(all_values, title, caption, y_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae28d79",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622355345Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(feature):\n",
    "    similarity_matrix = np.zeros((len(nodes_array), len(nodes_array))) \n",
    "    \n",
    "    for i in tqdm(range(similarity_matrix.shape[0])):\n",
    "        for j in range(i):\n",
    "            node_id1 = str(nodes_array[i])\n",
    "            node_id2 = str(nodes_array[j])            \n",
    "            similarity_matrix[i][j] = calculate_similarity(node_id1, node_id2, feature)\n",
    "        \n",
    "    similarity_matrix = similarity_matrix + similarity_matrix.transpose()\n",
    "    \n",
    "    #Year is the only feature that is initially distance and needs to be similarity\n",
    "    if feature == 'year':\n",
    "        similarity_matrix = np.ones_like(similarity_matrix) - similarity_matrix/np.amax(similarity_matrix)\n",
    "        similarity_matrix = np.where(similarity_matrix > 1, -1, similarity_matrix)\n",
    "        \n",
    "    return similarity_matrix\n",
    "\n",
    "year_similarity_matrix = calculate_similarity_matrix('year')\n",
    "np.save('year_similarity_matrix.npy', year_similarity_matrix)\n",
    "\n",
    "mesh_jaccard_similarity_matrix = calculate_similarity_matrix('mesh_jaccard')\n",
    "np.save('mesh_jaccard_similarity_matrix.npy', mesh_jaccard_similarity_matrix)\n",
    "\n",
    "\n",
    "mesh_tree_similarity_matrix = calculate_similarity_matrix('mesh_tree')\n",
    "mesh_tree_similarity_matrix /= max_mesh_overlap\n",
    "np.save('mesh_tree_similarity_matrix.npy', mesh_tree_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# bib_coupling_similarity_matrix = calculate_similarity_matrix('bib-coupling')\n",
    "# np.save('bib_coupling_similarity_matrix.npy', bib_coupling_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# cocitation_similarity_matrix = calculate_similarity_matrix('co-citation')\n",
    "# np.save('cocitation_similarity_matrix.npy', cocitation_similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# chemical_similarity_matrix = calculate_similarity_matrix('chemical')\n",
    "# np.save('chemical_similarity_matrix.npy', chemical_similarity_matrix)\n",
    "\n",
    "\n",
    "# grants_similarity_matrix = calculate_similarity_matrix('grants')\n",
    "# np.save('grants_similarity_matrix.npy', grants_similarity_matrix)\n",
    "\n",
    "\n",
    "# journal_similarity_matrix = calculate_similarity_matrix('journal')\n",
    "# np.save('journal_similarity_matrix.npy', journal_similarity_matrix)\n",
    "\n",
    "\n",
    "# keyword_similarity_matrix = calculate_similarity_matrix('keyword')\n",
    "# np.save('keyword_similarity_matrix.npy', keyword_similarity_matrix)\n",
    "\n",
    "\n",
    "# author_similarity_matrix = calculate_similarity_matrix('author')\n",
    "# np.save('author_similarity_matrix.npy', author_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67cc47",
   "metadata": {},
   "source": [
    "<h1>Load metrices </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bb696",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622624993Z"
    }
   },
   "outputs": [],
   "source": [
    "year_similarity_matrix = np.load('year_similarity_matrix.npy')\n",
    "mesh_tree_similarity_matrix = np.load('mesh_tree_similarity_matrix.npy')\n",
    "bib_coupling_similarity_matrix = np.load('bib_coupling_similarity_matrix.npy')\n",
    "cocitation_matrix = np.load('cocitation_matrix.npy')\n",
    "cocitation_matrix/= np.amax(cocitation_matrix)\n",
    "chemical_similarity_matrix = np.load('chemical_similarity_matrix.npy')\n",
    "journal_similarity_matrix = np.load('journal_similarity_matrix.npy')\n",
    "aggregated_three_hop_similarity = np.load('aggregated_three_hop_similarity.npy')\n",
    "\n",
    "author_similarity_matrix = np.load('author_similarity_matrix.npy')\n",
    "keyword_similarity_matrix = np.load('keyword_similarity_matrix.npy')\n",
    "\n",
    "\n",
    "# cocitation_matrix = np.load('cocitation_matrix.npy')\n",
    "# bib_coupling_matrix = np.load('bib_coupling_matrix.npy')\n",
    "\n",
    "# cocitation_matrix /= np.amax(cocitation_matrix)\n",
    "# bib_coupling_matrix /= np.amax(bib_coupling_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad3cec",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622684966Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "title = 'Year similarity boxplot'\n",
    "caption = 'Year similarity value is defined by year difference devided by maximum difference'\n",
    "y_axis = 'Year similarity'\n",
    "report_matrix_stats(year_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "title = 'Jaccard mesh similarity boxplot'\n",
    "caption = 'Jaccard mesh similarity value is defined by \\n the size of union of two mesh terms devided by the size of their intersection'\n",
    "y_axis = 'Jaccard mesh similarity'\n",
    "report_matrix_stats(mesh_jaccard_similarity_matrix,  title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "title = 'Max-Normalized pairwise mean mesh-tree similarity boxplot'\n",
    "caption = 'Max-Normalized pairwise mesh-tree similarity value is defined by \\n the average of pairwise MeSH similarities of papers'\n",
    "y_axis = 'Max-Normalized pairwise mesh-tree similarity'\n",
    "report_matrix_stats(mesh_tree_similarity_matrix,  title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "\n",
    "title = 'Jaccard chemical similarity boxplot'\n",
    "caption = 'Jaccard chemical similarity value is defined by \\n the size of union of two chemical lists devided by the size of their intersection'\n",
    "y_axis = 'Jaccard chemical similarity'\n",
    "report_matrix_stats(chemical_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "# title = 'Jaccard co-citation similarity boxplot'\n",
    "# caption = 'Jaccard co-citation similarity value is defined by \\n the size of union of inward citation list devided by the size of their intersection'\n",
    "# y_axis = 'Jaccard co-citation similarity'\n",
    "# report_matrix_stats(cocitation_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "title = 'Max-normalized co-citation similarity boxplot'\n",
    "caption = 'Max-normalized co-citation similarity value is defined by \\n the frequency of co-citation devided by maximum co-citation value observed'\n",
    "y_axis = 'Max-normalized co-citation similarity'\n",
    "report_matrix_stats(cocitation_matrix,  title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "title = 'Jaccard bib-coupling similarity boxplot'\n",
    "caption = 'Jaccard bib-coupling similarity value is defined by \\n the size of union of outward citation list devided by the size of their intersection'\n",
    "y_axis = 'Jaccard bib-coupling similarity'\n",
    "report_matrix_stats(bib_coupling_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "# title = 'Max-normalized bib-coupling similarity boxplot'\n",
    "# caption = 'Max-normalized bib-coupling similarity value is defined by \\n the frequency of bib-coupling devided by maximum bibcoupling value observed'\n",
    "# y_axis = 'Max-normalized bib-coupling similarity'\n",
    "# report_matrix_stats(bib_coupling_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "title = 'Three-hop range similarity boxplot'\n",
    "caption = 'Three-hop range similarity value is defined in an undirected graph, \\n as 0.7 if two nodes are neighbours, 0.2 if their distance is 2, 0.1 if their distance is 3 \\n and 0 otherwise'\n",
    "y_axis = 'Three-hop range similarity'\n",
    "report_matrix_stats(aggregated_three_hop_similarity, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "title = 'Grant similarity boxplot'\n",
    "caption = 'Grant similarity value is defined as 1 if two nodes have listed the same grant and 0 otherwise'\n",
    "y_axis = 'Grant similarity'\n",
    "report_matrix_stats(grants_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "title = 'Journal similarity boxplot'\n",
    "caption = 'Journal similarity value is defined as 1 if two nodes are published in a same journal and 0 otherwise'\n",
    "y_axis = 'Journal similarity'\n",
    "report_matrix_stats(journal_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "title = 'Jaccard keyword similarity boxplot'\n",
    "caption = 'Keyword similarity value is defined as jaccard similarity of two keywords if existed \\n else this similarity is not taken into account'\n",
    "y_axis = 'Keyword similarity'\n",
    "report_matrix_stats(keyword_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "\n",
    "title = 'Jaccard similarity of authors'\n",
    "caption = 'Jaccard similarity value is defined by \\n the size of union of inward citation list devided by the size of their intersection'\n",
    "y_axis = 'Jaccard author similarity'\n",
    "report_matrix_stats(author_similarity_matrix, title =title, caption=caption, y_axis=y_axis)\n",
    "\n",
    "title = 'Pairwise similarity'\n",
    "caption = 'Pairwise distance similarity value is defined by \\n considering weighted similarities of different features'\n",
    "y_axis = 'Pairwise distance similarity'\n",
    "report_matrix_stats(cluster_pairwise_similarity, title =title, caption=caption, y_axis=y_axis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9a7ed",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622736713Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(grants_similarity_matrix))\n",
    "\n",
    "for key, value in metadata_dict.items():\n",
    "    if len(value['grants']) > 0:\n",
    "        print(value['grants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea75bd3",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622786998Z"
    }
   },
   "outputs": [],
   "source": [
    "#Verify that those entries without grant actually do not have grants\n",
    "sample_list_of_pmid_without_grant = []\n",
    "without_grants_doi = []\n",
    "i = 0\n",
    "with_grants = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if 'grant' in str(value).lower():\n",
    "        with_grants.append(key)\n",
    "    \n",
    "    else:\n",
    "        sample_list_of_pmid_without_grant.append(key)\n",
    "        without_grants_doi.append(doi_lookup_dict[key])\n",
    "        \n",
    "    i+= 1\n",
    "    \n",
    "    if i == 100:\n",
    "        break\n",
    "\n",
    "print('Statistics for first 10 papers:\\n')\n",
    "print(f'DOIs without grant: {without_grants_doi[0:10]}')\n",
    "print(f'PMIDs without grant: {sample_list_of_pmid_without_grant[0:10]}\\n')\n",
    "\n",
    "print(f'PMID of papers with grants {with_grants}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd746ba",
   "metadata": {},
   "source": [
    "<h1>Example cross-checking of bib-coupling and co-citation</h1>\n",
    "An example of bib-couple and co-citation calculation for two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784fe88",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622839577Z"
    }
   },
   "outputs": [],
   "source": [
    "first_node_id = 0\n",
    "first_node_pmid = nodes_array[first_node_id]\n",
    "\n",
    "second_node_id = 50\n",
    "second_node_pmid = nodes_array[second_node_id]\n",
    "\n",
    "first_reference = {}\n",
    "first_cited = {}\n",
    "\n",
    "second_reference = {}\n",
    "second_cited = {}\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if start_node == first_node_pmid:\n",
    "        first_reference[end_node] = 1\n",
    "    if end_node == first_node_pmid:\n",
    "        first_cited[start_node] = 1     \n",
    "        \n",
    "    \n",
    "    if start_node == second_node_pmid:\n",
    "        second_reference[end_node] = 1\n",
    "    if end_node == second_node_pmid:\n",
    "        second_cited[start_node] = 1       \n",
    "\n",
    "print(f'recorded bib-coupling: {bib_coupling_matrix[first_node_id][second_node_id]}')\n",
    "print(f'crossed checked bib-coupling: {calculate_dict_similarity(first_reference, second_reference, mode = \"number_of_common_terms\")}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'recorded co-citation: {cocitation_matrix[first_node_id][second_node_id]}')\n",
    "print(f'crossed checked co-citation: {calculate_dict_similarity(first_cited, second_cited, mode = \"number_of_common_terms\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d49b37",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "<h1>Missing ISSN</h1>\n",
    "Finding xml's without ISSN. Any journal info?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea48bb",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622891164Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_issn = []\n",
    "for key, value in all_xmls.items():\n",
    "    if not 'issn' in str(value).lower():\n",
    "        missing_issn.append(key)\n",
    "        \n",
    "print(f'missing ISSN pmids {missing_issn}')\n",
    "print(len(missing_issn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab282043",
   "metadata": {},
   "source": [
    "<h1>Examples of co-citation similarity of 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4b7d8",
   "metadata": {},
   "source": [
    "Interestingly, for all those of paris with jaccard_cocitation = 1, they have been only cited once! As we see below, citation counts of all of those pairs is 1. This could falsly inflate co_citation similarity. This will likely be the case with new publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff64255",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622942901Z"
    }
   },
   "outputs": [],
   "source": [
    "year_similarity_matrix = np.load('year_similarity_matrix.npy')\n",
    "year_similarity_array = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "for i in range(cocitation_similarity_matrix.shape[0]):\n",
    "    for j in range(i):\n",
    "        if cocitation_similarity_matrix[i][j] > 0.99:\n",
    "            row_indices.append(i)\n",
    "            col_indices.append(j)\n",
    "\n",
    "for idx in range(len(row_indices)):\n",
    "    row = row_indices[idx]\n",
    "    col = col_indices[idx]\n",
    "    year_similarity_array.append(year_similarity_matrix[row][col])\n",
    "    print(f'citation count of both papers: {np.sum(adj_matrix[:,row])} cocitation Jaccard similarity: {cocitation_similarity_matrix[row][col]} Pmid1:{nodes_array[row]} Pmid1:{nodes_array[col]}')\n",
    "\n",
    "\n",
    "print(f'Average year similarity {np.average(year_similarity_array)} total cases {len(row_indices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b73db1",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.622993186Z"
    }
   },
   "outputs": [],
   "source": [
    "bib_coupling_similarity_matrix = np.load('bib_coupling_similarity_matrix.npy')\n",
    "cocitation_similarity_matrix = np.load('cocitation_similarity_matrix.npy')\n",
    "\n",
    "#For co_citation of 1\n",
    "first_paper_id = np.unravel_index(cocitation_similarity_matrix.argmax(), cocitation_similarity_matrix.shape)[0]\n",
    "second_paper_id = np.unravel_index(cocitation_similarity_matrix.argmax(), cocitation_similarity_matrix.shape)[1]\n",
    "\n",
    "first_node_pmid =  nodes_array[first_paper_id]\n",
    "second_node_pmid =  nodes_array[second_paper_id]\n",
    "\n",
    "first_cited = []\n",
    "second_cited = []\n",
    "\n",
    "\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if end_node == first_node_pmid:\n",
    "        first_cited.append(start_node)   \n",
    "        \n",
    "    if end_node == second_node_pmid:\n",
    "        second_cited.append(start_node)\n",
    "        \n",
    "\n",
    "print(f'first paper pmid: {first_node_pmid} second paper pmid: {second_node_pmid}. Citing list:\\n')\n",
    "print(first_cited)\n",
    "\n",
    "#Assert two arrays are equal\n",
    "assert np.sum(np.sort(first_cited) - np.sort(second_cited)) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f863b",
   "metadata": {},
   "source": [
    "<h1>Examples of bib-coupling similarity of 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c1dcd",
   "metadata": {},
   "source": [
    "Here the situation is differen. As we expect, reference list is typically larger than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc92ca",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.623046245Z"
    }
   },
   "outputs": [],
   "source": [
    "row_indices, col_indices = np.where(bib_coupling_similarity_matrix == 1)\n",
    "year_similarity_array = []\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "\n",
    "\n",
    "for i in range(bib_coupling_similarity_matrix.shape[0]):\n",
    "    for j in range(i):\n",
    "        if bib_coupling_similarity_matrix[i][j] > 0.99:\n",
    "            row_indices.append(i)\n",
    "            col_indices.append(j)\n",
    "            \n",
    "            \n",
    "for idx in range(len(row_indices)):\n",
    "    row = row_indices[idx]\n",
    "    col = col_indices[idx]\n",
    "    year_similarity_array.append(year_similarity_matrix[row][col])\n",
    "    print(f' Reference count of both papers: {np.sum(adj_matrix[row, :])} Bib-coupling Jaccard similarity: {bib_coupling_similarity_matrix[row][col]} Pmid1:{nodes_array[row]} Pmid1:{nodes_array[col]}')\n",
    "#print(f'citation count of both papers: {np.sum(adj_matrix[:,row])} Pmid1:{nodes_array[row]} Pmid1:{nodes_array[col]}')\n",
    "\n",
    "print(f'Average year similarity {np.average(year_similarity_array)}, total cases: {len(row_indices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239f8b0",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.623097211Z"
    }
   },
   "outputs": [],
   "source": [
    "bib_coupling_similarity_matrix = np.load('bib_coupling_similarity_matrix.npy')\n",
    "cocitation_similarity_matrix = np.load('cocitation_similarity_matrix.npy')\n",
    "\n",
    "#For co_citation of 1\n",
    "first_paper_id = row_indices[3]\n",
    "second_paper_id = col_indices[3]\n",
    "\n",
    "\n",
    "first_node_pmid =  3945365 #nodes_array[first_paper_id]\n",
    "second_node_pmid =  11916530 #nodes_array[second_paper_id]\n",
    "\n",
    "first_reference = []\n",
    "second_reference = []\n",
    "\n",
    "\n",
    "\n",
    "for edge in edge_array:\n",
    "    start_node = edge[0]\n",
    "    end_node = edge[1]\n",
    "\n",
    "    if start_node == first_node_pmid:\n",
    "        first_reference.append(end_node)   \n",
    "        \n",
    "    if start_node == second_node_pmid:\n",
    "        second_reference.append(end_node)\n",
    "        \n",
    "\n",
    "print(f'first paper pmid: {first_node_pmid} second paper pmid: {second_node_pmid}. total references: {len(first_reference)} Reference list:\\n')\n",
    "print(first_reference)\n",
    "\n",
    "#Assert two arrays are equal\n",
    "assert np.sum(np.sort(first_reference) - np.sort(second_reference)) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd51e3",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "<h1> Missing grants </h1>\n",
    "<p>Let's see how those samples with 'grant' string but no grant data look like </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483c43a",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.660967745Z"
    }
   },
   "outputs": [],
   "source": [
    "target_pmids = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if 'grantlist' in str(value).lower() and len(metadata_dict[key]['grants']) == 0:\n",
    "        target_pmids.append(key)\n",
    "        \n",
    "print(target_pmids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e1c17",
   "metadata": {},
   "source": [
    "Through better parsing, this number is now very low!\n",
    "Now the case of this 3:\n",
    "<html>\n",
    "<head>\n",
    "    <title>PMID and Reason for Missing</title>\n",
    "</head>\n",
    "<body>\n",
    "    <table border=\"1\">\n",
    "        <tr>\n",
    "            <th>PMID</th>\n",
    "            <th>Reason for Missing</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1971008</td>\n",
    "            <td>No grant Id, only agency and country</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2bbbec",
   "metadata": {},
   "source": [
    "<h1>Checking for same grants</h1> <p> Let's simply record number of same grants. It turns out no two papers have same grant. As we can see on the cell bellow, no two papers are supported by the same grant as the mean of the grant similarity matrix is 0</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fa89836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T18:08:01.055432022Z",
     "start_time": "2023-12-18T18:08:00.661040592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0024849361605998603\n"
     ]
    }
   ],
   "source": [
    "grants_similarity_matrix = np.load('grants_similarity_matrix.npy')\n",
    "print(np.mean(grants_similarity_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e105ed59",
   "metadata": {},
   "source": [
    "<h1>Missing Mesh</h1> \n",
    "As we verify manually, these 3 articles don't have any mesh term inside them. So, out of 1993 parsable responses, only 3 miss mesh information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97099845",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.683661854Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_xmls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-a54c7ac9a20e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtarget_pmids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mall_xmls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m\u001B[0;34m'meshheadinglist'\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_xmls\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m250\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mtarget_pmids\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'all_xmls' is not defined"
     ]
    }
   ],
   "source": [
    "target_pmids = []\n",
    "\n",
    "for key, value in all_xmls.items():\n",
    "    if not'meshheadinglist' in str(value).lower() and len(str(all_xmls[key])) > 250:\n",
    "        target_pmids.append(key)\n",
    "        \n",
    "print(target_pmids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69df6c",
   "metadata": {},
   "source": [
    "<h1>Highly repeated Chemicals </h1>\n",
    "<p> As we can see, in the first cluster, 96.32% of chemical terms are already in MesH terms </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a033c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.692851024Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "inside = 0\n",
    "outside = 0\n",
    "chemicals_not_in_MeSH = []\n",
    "\n",
    "for key, value in tqdm(metadata_dict.items()):\n",
    "    for chemical in value['chemical']:\n",
    "        if chemical in value['mesh']:\n",
    "            inside += 1\n",
    "        \n",
    "        else:\n",
    "            chemicals_not_in_MeSH.append(chemical)\n",
    "            outside += 1\n",
    "\n",
    "print(f'Number of whole chemicals in this cluster {inside + outside}')\n",
    "print(f'Percentage of Chemicals already in MeSH terms {(100*inside/(inside+outside)):.2f}')\n",
    "print(chemicals_not_in_MeSH, len(chemicals_not_in_MeSH))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d7f1c",
   "metadata": {},
   "source": [
    "<h1>Finding MeSH from UI </h1>\n",
    "<p> Currently we only have UI for MeSH terms or chemicals, but we want their tree structure to find their similarity. As it turns out, we can only get the MeSH tree for MeSH terms, not for chemicals </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b81f95",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733082479Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import multiprocessing \n",
    "\n",
    "def retrieve_mesh_tree(UI, idx, shared_dict):\n",
    "    url = f\"https://meshb.nlm.nih.gov/record/ui?ui={UI}\"\n",
    "    with requests.Session() as session:\n",
    "        url = f\"https://meshb.nlm.nih.gov/record/ui?ui={UI}\"\n",
    "\n",
    "        response = session.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        text_of_interest = response.text\n",
    "        \n",
    "        try:\n",
    "            treeNumber_idx = text_of_interest.index('treeNumber_0')\n",
    "            uniqueID = text_of_interest.index('Unique ID')\n",
    "            limited_string = text_of_interest[treeNumber_idx:uniqueID]\n",
    "            end_index = limited_string.find('</a>')\n",
    "            counter = 1\n",
    "\n",
    "            while end_index-counter + 1 >= 0:\n",
    "                if limited_string[end_index-counter] == '>':\n",
    "                    shared_dict[UI] = limited_string[end_index-counter + 1:end_index]\n",
    "                    break\n",
    "                counter += 1\n",
    "            \n",
    "        except:\n",
    "            shared_dict[UI] = \"failed\"\n",
    "\n",
    "    else:\n",
    "        shared_dict[UI] = \"failed\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp_mesh = {}\n",
    "all_mesh_terms = Manager().dict()\n",
    "\n",
    "lst = []\n",
    "\n",
    "\n",
    "for key, value in metadata_dict.items():\n",
    "    for mesh in value['mesh']:\n",
    "        if not mesh in all_mesh_terms:\n",
    "            temp_mesh[mesh] = 1\n",
    "            \n",
    "\n",
    "for index, mesh in enumerate(temp_mesh.keys()):\n",
    "    lst.append((mesh, index, all_mesh_terms))\n",
    "\n",
    "    \n",
    "CHUNK_SIZE = 50\n",
    "SPLIT = len(lst) // CHUNK_SIZE + 1\n",
    "\n",
    "\n",
    "for i in tqdm(range(SPLIT)):\n",
    "    if i != SPLIT -1:\n",
    "        with multiprocessing.Pool(processes=10) as pool:\n",
    "                pool.starmap(retrieve_mesh_tree, lst[i*CHUNK_SIZE: (i+1) * CHUNK_SIZE])\n",
    "    \n",
    "    else:\n",
    "        with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "            pool.starmap(retrieve_mesh_tree, lst[i*CHUNK_SIZE: ])  \n",
    "\n",
    "        \n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc791cd",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733191394Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "64\n",
    "        try:\n",
    "65\n",
    "            if 'JournalIssue' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal'].keys():\n",
    "66\n",
    "                if 'PubDate' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']:\n",
    "67\n",
    "                    if 'Year' in xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']:\n",
    "68\n",
    "                        pub_year = xml_dict['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Yimport json\n",
    "with open('all_mesh_terms.json', 'r') as json_file:\n",
    "    all_mesh_terms = json.load(json_file)\n",
    "    \n",
    "    \n",
    "mesh_save_dict = {}\n",
    "failed = 0\n",
    "success = 0\n",
    "\n",
    "for key, val in all_mesh_terms.items():\n",
    "    mesh_save_dict[key] = val\n",
    "    \n",
    "    if val == 'failed':\n",
    "        failed += 1\n",
    "    \n",
    "    else:\n",
    "        success += 1\n",
    "    \n",
    "print(success, failed)\n",
    "# with open('all_meshhh_terms.json', 'w') as json_file:\n",
    "#     json.dump(mesh_save_dict, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bcdf43",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "<h1> Finally we create our first aggregated similarity metric</h1>\n",
    "<p> The way we handle that is we create a set of weights for our metrics. In case of a missing value, the relative scales are adjusted. This is the initial proposed weights, ordered in ascending manner: </p>    \n",
    "<table border=\"1\">\n",
    "        <tr>\n",
    "            <th>Metric</th>\n",
    "            <th>Relative weight</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Year</td>\n",
    "            <td>1</td>\n",
    "        </tr>    \n",
    "        <tr>\n",
    "            <td>Journal similarity</td>\n",
    "            <td>2</td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "            <td>Max-normalized Cocitation</td>\n",
    "            <td>4</td>\n",
    "        </tr>    \n",
    "        <tr>\n",
    "            <td>Bib-coupling Jaccard</td>\n",
    "            <td>6</td>\n",
    "        </tr>       \n",
    "        <tr>\n",
    "            <td>Three-hop similarity</td>\n",
    "            <td>8</td>\n",
    "        </tr>        \n",
    "        <tr>\n",
    "            <td>Chemical Jaccard</td>\n",
    "            <td>10</td>\n",
    "        </tr>     \n",
    "        <tr>\n",
    "            <td>MeSH Jaccard</td>\n",
    "            <td>15</td>\n",
    "        </tr>       \n",
    "    </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17509e77",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733308374Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "reference_weights = {'year': 1, 'journal': 2, 'cocitation': 4, 'bib-coupling': 6, \n",
    "                     'three-hop': 8, 'chemical': 10, 'mesh':15}\n",
    "\n",
    "initial_weight_sum = 0\n",
    "\n",
    "for _, value in reference_weights.items():\n",
    "    initial_weight_sum += value\n",
    "    \n",
    "    \n",
    "reference_matrices = {'year': year_similarity_matrix, 'journal': journal_similarity_matrix,\n",
    "                     'cocitation': cocitation_matrix, 'bib-coupling': bib_coupling_similarity_matrix, \n",
    "                     'three-hop': aggregated_three_hop_similarity , 'chemical': chemical_similarity_matrix,\n",
    "                     'mesh':mesh_tree_similarity_matrix}\n",
    "\n",
    "def calculate_two_paper_similarity(paper1_index, paper2_index):\n",
    "    #paper1_index and paper2_index are those mapped to interval [1,2,...,n]\n",
    "    \n",
    "    total_similarity = 0\n",
    "    sum_of_available_feature_weights = 0\n",
    "    \n",
    "    for key, matrix_of_interest in reference_matrices.items():\n",
    "        if matrix_of_interest[paper1_index][paper2_index] >= 0:\n",
    "            sum_of_available_feature_weights += reference_weights[key]\n",
    "            total_similarity += reference_weights[key]*matrix_of_interest[paper1_index][paper2_index]\n",
    "    \n",
    "    if sum_of_available_feature_weights == 0:\n",
    "        return 0\n",
    "    \n",
    "    return total_similarity * (initial_weight_sum/sum_of_available_feature_weights)\n",
    "\n",
    "\n",
    "def calculate_whole_cluster_similarity():\n",
    "    cluster_pairwise_similarity = np.zeros_like(adj_matrix)\n",
    "    \n",
    "    for i in tqdm.tqdm(range(cluster_pairwise_similarity.shape[0])):\n",
    "        for j in range(i):\n",
    "            cluster_pairwise_similarity[i][j] = calculate_two_paper_similarity(i,j)\n",
    "    \n",
    "    return cluster_pairwise_similarity + cluster_pairwise_similarity.transpose()\n",
    "\n",
    "cluster_pairwise_similarity = calculate_whole_cluster_similarity()/initial_weight_sum\n",
    "np.save('cluster_pairwise_similarity.npy', cluster_pairwise_similarity)\n",
    "\n",
    "\n",
    "title = 'Pairwise similarity'\n",
    "caption = 'Pairwise similarity value is defined by \\n considering weighted similarities of different features'\n",
    "y_axis = 'Pairwise similarity'\n",
    "report_matrix_stats(cluster_pairwise_similarity, title =title, caption=caption, y_axis=y_axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfc0e4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733366724Z"
    }
   },
   "outputs": [],
   "source": [
    "for key, value in reference_matrices.items():\n",
    "    print(key, np.amax(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa61e4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733422259Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load stuff\n",
    "\n",
    "import pickle\n",
    "print(len(nodes_array))\n",
    "file_name = \"nodes_array.pkl\"\n",
    "\n",
    "# Open the file in binary write mode\n",
    "with open(file_name, 'rb') as file:\n",
    "    # Use pickle.load() to deserialize and load the object\n",
    "    nodes_array = pickle.load(file)\n",
    "    \n",
    "print(len(nodes_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d8420",
   "metadata": {},
   "source": [
    "<h3> Checking with cluster 18, which is the cluster of interest </h3>\n",
    "<p> Initially we started with cluster_1, but it turns out that cluster_18 is the coherent cluster with microRNA theme. Let's investigate if that's the case that these two clusters are different?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1c591",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733480168Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = 'fabios_test_data_doi.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "# print(df.head())\n",
    "cluster_18 = []\n",
    "\n",
    "cluster_18_doi_lookup = {}\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    cluster_id, node_id, doi = row['cluster_id'], row['node_id'], row['doi']\n",
    "    \n",
    "    if int(cluster_id) == 18:\n",
    "        cluster_18.append((node_id,doi))\n",
    "        cluster_18_doi_lookup[str(node_id)] = doi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b56eb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733534520Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "common = 0 \n",
    "\n",
    "for element in tqdm(cluster_18):\n",
    "    node_id , doi = element\n",
    "    \n",
    "    \n",
    "    #This one doesn't have pmid \n",
    "    if node_id == 14482471:\n",
    "        continue\n",
    "        \n",
    "    if int(node_id) in nodes_array and doi == doi_lookup_dict[str(node_id)]:\n",
    "        common += 1\n",
    "        \n",
    "        assert node_id == nodes_array[nodes_array.index(node_id)]\n",
    "        \n",
    "\n",
    "print(common)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52be43",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733598129Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cluster_18[0:10])\n",
    "while True:\n",
    "    pmid = input('Enter pmid')\n",
    "    print(f'cluster_18: {cluster_18_doi_lookup[pmid]} cluster_1: {doi_lookup_dict[pmid]} equal? {cluster_18_doi_lookup[pmid] == doi_lookup_dict[pmid]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae769475",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733799188Z"
    }
   },
   "outputs": [],
   "source": [
    "print(doi_lookup_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128c1b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-18T18:08:00.733860603Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
