{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a99bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T16:00:09.875236673Z",
     "start_time": "2023-12-22T16:00:09.830206154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi\n"
     ]
    }
   ],
   "source": [
    "print(\"vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a490ae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T16:00:09.919734160Z",
     "start_time": "2023-12-22T16:00:09.839310222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: relation \"hm31.pubmed_all_xmls_abstracts\" does not exist\n",
      "LINE 1: INSERT INTO hm31.pubmed_all_xmls_abstracts VALUES (123425,''...\n",
      "                    ^\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Set the PGDATABASE environment variable\n",
    "os.environ[\"PGDATABASE\"] = \"ernieplus\"\n",
    "\n",
    "def insert_values_into_table(values_list):\n",
    "    try:\n",
    "        # Connect to the PostgreSQL database\n",
    "        conn = psycopg2.connect(\"\")\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        \n",
    "        args = ','.join(cur.mogrify(\"(%s,%s,%s, %s, %s, %s, %s, %s, %s, %s)\", i).decode('utf-8')\n",
    "                for i in values_list)\n",
    "        \n",
    "        cur.execute(\"INSERT INTO hm31.pubmed_all_xmls_abstracts VALUES \" + (args))\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"Inserted values successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "# Example usage:\n",
    "values_to_insert = [\n",
    "    (123425, '', '', '', 0, 0, '', 'doi1','title1', 'abs1'),\n",
    "    (543251, '5678-1234', 'Grant3, Grant4', 'Chemical3, Chemical4', 2021, 2020, 'Term3, Term4','doi2','t1','a1')\n",
    "]\n",
    "\n",
    "insert_values_into_table(values_to_insert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba8ba960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T16:00:50.484762591Z",
     "start_time": "2023-12-22T16:00:09.879557983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read\n",
      "parsed\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import pandas as pd\n",
    "\n",
    "def parse(file_path):\n",
    "    try:\n",
    "        # Open and read the XML file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            xml_contents = file.read()\n",
    "        print(\"read\")\n",
    "        # Parse the XML content using xmltodict\n",
    "        xml_dict = xmltodict.parse(xml_contents)\n",
    "        print(\"parsed\")\n",
    "        # Print the parsed XML as a Python dictionary\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        \n",
    "    return xml_dict\n",
    "\n",
    "xml_dict = parse('/shared/hossein_hm31/xml_data/pubmed23n0001.xml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16cbb502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T16:00:50.529702791Z",
     "start_time": "2023-12-22T16:00:50.484429935Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'metapub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fa7a0f2f89fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmetapub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'convert pmid2doi 10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'metapub'"
     ]
    }
   ],
   "source": [
    "import metapub\n",
    "\n",
    "!convert pmid2doi 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef3df4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T16:00:50.532584843Z",
     "start_time": "2023-12-22T16:00:50.531521923Z"
    }
   },
   "outputs": [],
   "source": [
    "from dicttoxml import dicttoxml\n",
    "xml = dicttoxml(xml_dict['PubmedArticleSet']['PubmedArticle'][0])\n",
    "print(type(xml))\n",
    "# print(str(xml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0006f3d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.534953489Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(xml_dict['PubmedArticleSet']['PubmedArticle']))\n",
    "# print(xml_dict['PubmedArticleSet']['PubmedArticle'][0])\n",
    "print(type(xml_dict['PubmedArticleSet']['PubmedArticle'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f257ed",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.536857511Z"
    }
   },
   "outputs": [],
   "source": [
    "article_id_list = temp['PubmedData']['ArticleIdList']['ArticleId']\n",
    "print('doi' in str(article_id_list[1]))\n",
    "# print(article_id_list[1])\n",
    "print(article_id_list[1].keys())\n",
    "print(article_id_list[1]['#text'])\n",
    "\n",
    "\n",
    "# Iterate through the list of ArticleId entries\n",
    "# for article_id in article_id_list['ArticleId']:\n",
    "#     if article_id.get('@IdType') == 'doi':\n",
    "#         doi = article_id['#text']\n",
    "#         print(\"DOI:\", doi)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5a03d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.541980299Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text = \"\"\"\n",
    "<Day>16</Day>\n",
    "</PubMedPubDate>\n",
    "<PubMedPubDate PubStatus=\"medline\">\n",
    "<Year>1976</Year>\n",
    "<Month>3</Month>\n",
    "<Day>16</Day>\n",
    "<Hour>0</Hour>\n",
    "<Minute>1</Minute>\n",
    "</PubMedPubDate>\n",
    "<PubMedPubDate PubStatus=\"entrez\">\n",
    "<Year>1976</Year>\n",
    "<Month>3</Month>\n",
    "<Day>16</Day>\n",
    "<Hour>0</Hour>\n",
    "<Minute>0</Minute>\n",
    "</PubMedPubDate>\n",
    "</History>\n",
    "<PublicationStatus>ppublish</PublicationStatus>\n",
    "<ArticleIdList>\n",
    "<ArticleId IdType=\"pubmed\">4311</ArticleId>\n",
    "<ArticleId IdType=\"doi\">10.1111/j.1432-1033.1976.tb10218.x</ArticleId>\n",
    "</ArticleIdList>\n",
    "</PubmedData>\n",
    "</PubmedArticle>\n",
    "</PubmedArticleSet>\n",
    "\"\"\"\n",
    "\n",
    "doi_pattern = r'\\b10\\.\\d{4,}/\\S+'\n",
    "doi_pattern = r'\\b10\\.\\d{4,}(?:\\.\\d+)*\\/[^\\s]+'\n",
    "\n",
    "\n",
    "# Find all DOIs in the text\n",
    "dois = re.findall(doi_pattern, text)\n",
    "\n",
    "# Print the found DOIs\n",
    "for doi in dois:\n",
    "    print(\"Found DOI:\", doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886dbc6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.585416669Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def find_doi(string):\n",
    "    doi_pattern = r'\\b10\\.\\d{4,}(?:\\.\\d+)*\\/[^\\s]+'\n",
    "    dois = re.findall(doi_pattern, string)\n",
    "    return dois\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa0a21",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.585700163Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_abstract(abstract):\n",
    "    overhauled = ''\n",
    "    \n",
    "    if type(abstract) != str:    \n",
    "      for a in abstract:\n",
    "        if type(a) != str:\n",
    "            if a != None and '#text' in a:\n",
    "                overhauled += a['#text']\n",
    " \n",
    "        else:\n",
    "            overhauled += a\n",
    "        \n",
    "      return overhauled\n",
    "\n",
    "    return abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620b8e6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.585971573Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def parse_single_record(xml_dict, originial_dict, i=0):\n",
    "    mesh_headings = []\n",
    "    grants = []\n",
    "    year = \"\"\n",
    "    journal_ISSN = \"\"\n",
    "    chemical_list = []\n",
    "    meta_data = {}\n",
    "    PMID = \"\"\n",
    "    doi = \"\"\n",
    "    title = \"\"\n",
    "    abstract = \"\"\n",
    "\n",
    "    year_revised = \"\"\n",
    "    month_revised = \"\"\n",
    "    day_revised = \"\"\n",
    "    date_revised = \"\"\n",
    "    \n",
    "    journal_puddate = \"\"\n",
    "\n",
    "    try:\n",
    "        xml_dict = dict(xml_dict)\n",
    "\n",
    "        #         print(xml_dict)\n",
    "\n",
    "        try:\n",
    "            if 'PMID' in xml_dict:\n",
    "                PMID = xml_dict['PMID']['#text']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if 'DateRevised' in xml_dict:\n",
    "\n",
    "                new_dic = dict(xml_dict['DateRevised'])\n",
    "\n",
    "\n",
    "                if 'Year' in new_dic:\n",
    "                    year_revised = new_dic['Year']\n",
    "\n",
    "                if 'Month' in new_dic:\n",
    "                    month_revised = new_dic['Month']\n",
    "\n",
    "                if 'Day' in new_dic:\n",
    "                    day_revised = new_dic['Day']\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "       \n",
    "\n",
    "        try:\n",
    "            if 'Article' in xml_dict:\n",
    "                new_dic = dict(xml_dict['Article'])\n",
    "                journal_ISSN = new_dic['Journal']['ISSN']['#text']\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if 'Article' in xml_dict:\n",
    "                new_dic = dict(xml_dict['Article'])\n",
    "                new_dic = str(new_dic['Journal']['JournalIssue']['PubDate'])\n",
    "                \n",
    "                pattern = re.compile(r'\\d{4}')\n",
    "                year = pattern.findall(new_dic)[0]\n",
    "\n",
    "   \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "\n",
    "            article_id_list = originial_dict['PubmedData']['ArticleIdList']['ArticleId']\n",
    "\n",
    "            for element in article_id_list:\n",
    "                if 'doi' in str(element).lower():\n",
    "                    doi = element['#text']\n",
    "                    break\n",
    "        #                 print('doi' in str(article_id_list[1]))\n",
    "        #                 # print(article_id_list[1])\n",
    "        #                 print(article_id_list[1].keys())\n",
    "        #                 print(article_id_list[1]['#text'])\n",
    "        #                 #print('doi' in str(article_id_list[1]))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if 'Article' in xml_dict:\n",
    "                new_dic = dict(xml_dict['Article'])\n",
    "\n",
    "            if 'Title' in new_dic['Journal'].keys():\n",
    "                journal_title = new_dic['Journal']['Title']\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if 'Article' in xml_dict:\n",
    "                new_dic = dict(xml_dict['Article'])\n",
    "\n",
    "                title = new_dic['ArticleTitle']\n",
    "\n",
    "                #                 if i == 10844:\n",
    "                #                     print(title,'AAA \\n')\n",
    "\n",
    "                if '#text' in title:\n",
    "                    title = title['#text']\n",
    "\n",
    "                #                     if i == 10844:\n",
    "                #                         print(title,'BBB \\n')\n",
    "\n",
    "                elif 'b' in title:\n",
    "                    title = title['b']\n",
    "                    if '#text' in title:\n",
    "                        title = title['#text']\n",
    "\n",
    "\n",
    "                elif 'sup' in title:\n",
    "                    title = title['sup']\n",
    "\n",
    "                if 'i' in title:\n",
    "                    title = title['i']\n",
    "\n",
    "                if type(title) == list:\n",
    "                    title = ' '.join(title)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "\n",
    "            if 'Article' in xml_dict: \n",
    "                new_dic = dict(xml_dict['Article'])\n",
    "                abstract = new_dic['Abstract']['AbstractText']\n",
    "                \n",
    "                while '#text' in abstract:\n",
    "                    abstract = abstract['#text']\n",
    "                                        \n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if 'Article' in xml_dict:\n",
    "                new_dic = dict(xml_dict['Article'])\n",
    "\n",
    "            if 'GrantList' in new_dic:\n",
    "                if type(new_dic['GrantList']['Grant']) == list:\n",
    "                    for grant in new_dic['GrantList']['Grant']:\n",
    "                        if 'GrantID' in grant:\n",
    "                            grants.append((grant['GrantID']))\n",
    "\n",
    "                else:\n",
    "                    grants.append(new_dic['GrantList']['Grant']['GrantID'])\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if 'MeshHeadingList' in xml_dict:\n",
    "                if type(xml_dict['MeshHeadingList']['MeshHeading']) == list:\n",
    "                    for mesh in xml_dict['MeshHeadingList']['MeshHeading']:\n",
    "                        if '@Type' in mesh['DescriptorName'].keys() and mesh['DescriptorName']['@Type'] == 'Geographic':\n",
    "                            continue\n",
    "\n",
    "                        mesh_headings.append((mesh['DescriptorName']['@UI']))\n",
    "\n",
    "                else:\n",
    "\n",
    "                    mesh = xml_dict['MeshHeadingList']['MeshHeading']['DescriptorName']\n",
    "                    if (not '@Type' in mesh.keys() or mesh['@Type'] != 'Geographic'):\n",
    "                        mesh_headings.append(xml_dict['MeshHeadingList']['MeshHeading']['DescriptorName']['@UI'])\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "\n",
    "            if 'ChemicalList' in xml_dict:\n",
    "                if type(xml_dict['ChemicalList']['Chemical']) == list:\n",
    "                    for substance in xml_dict['ChemicalList']['Chemical']:\n",
    "                        if substance['NameOfSubstance']['@UI'][0].lower() == 'c':\n",
    "                            chemical_list.append(substance['NameOfSubstance']['@UI'])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if xml_dict['ChemicalList']['Chemical']['NameOfSubstance']['@UI'][0].lower() == 'c':\n",
    "                        chemical_list.append(xml_dict['ChemicalList']['Chemical']['NameOfSubstance']['@UI'])\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"ERR\")\n",
    "            pass\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    if title == None:\n",
    "        title = ''\n",
    "\n",
    "    if abstract == None:\n",
    "        abstract = ''\n",
    "\n",
    "    abstract = clean_abstract(abstract)\n",
    "\n",
    "    if year_revised == '':\n",
    "        date_revised = '0000-00-00'\n",
    "\n",
    "    else:\n",
    "        date_revised = f'{year_revised}-{month_revised}-{day_revised}'\n",
    "        # print(date_revised)\n",
    "\n",
    "    meta_data = {'PMID': int(PMID), 'mesh': str(mesh_headings), 'grants': str(grants), 'year': str(year),\n",
    "                 'journal_ISSN': str(journal_ISSN), 'journal_title': str(journal_title),\n",
    "                 'chemical': str(chemical_list), 'doi': doi.lower(),\n",
    "                 'title': str(title), 'abstract': str(abstract),\n",
    "                 'date_revised': date_revised}\n",
    "\n",
    "    return meta_data\n",
    "\n",
    "meta_data_array = []\n",
    "\n",
    "counter = 0\n",
    "for i in tqdm(range(len(xml_dict['PubmedArticleSet']['PubmedArticle']))):\n",
    "    \n",
    "    rec = xml_dict['PubmedArticleSet']['PubmedArticle'][i]['MedlineCitation']\n",
    "    total = xml_dict['PubmedArticleSet']['PubmedArticle'][i]\n",
    "    x=parse_single_record(rec, total,i)\n",
    "    meta_data_array.append(x)\n",
    "    \n",
    "    if x['abstract'] == '':\n",
    "        counter += 1\n",
    "    \n",
    "    \n",
    "    if type(x['title']) != str:\n",
    "        print(x['title'])\n",
    "        print(type(x['title']))\n",
    "        print(i)\n",
    "\n",
    "print(counter)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd946cf",
   "metadata": {},
   "source": [
    "<h2>Saving into parquet </h2>\n",
    "<p> We will convert the metadata array to dataframe, write it to parquet and read it and inspect the saved file against the original array as a cross-check mechanism</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e657615",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.586293489Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(meta_data_array)\n",
    "df.set_index(['PMID', 'doi'], inplace=True)\n",
    "df.head()\n",
    "df.to_parquet('myfile.parquet')\n",
    "print(df.head(2))\n",
    "!rm myfile.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476b193",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.586685306Z"
    }
   },
   "outputs": [],
   "source": [
    "read_df = pd.read_parquet('myfile.parquet')\n",
    "print(read_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97cbbc",
   "metadata": {},
   "source": [
    "<h2>As we see here, very few dois is available in the XMLS. This is cross checked with looking for 'doi' occurence in the string</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675ae1d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.586803018Z"
    }
   },
   "outputs": [],
   "source": [
    "def investigate_for_doi_manually(xml_dict):\n",
    "    xml = str(dicttoxml(xml_dict))\n",
    "    return 'doi' in xml\n",
    " \n",
    "total_doi = 0\n",
    "for idx, meta in tqdm(enumerate(meta_data_array),  total = len(meta_data_array)):\n",
    "    if meta['doi'] != '':\n",
    "        total_doi += 1\n",
    "\n",
    "print(f'totaly {total_doi} dois available from {len(meta_data_array)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d694450",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.586914808Z"
    }
   },
   "outputs": [],
   "source": [
    "# total_doi = 0\n",
    "for idx, meta in tqdm(enumerate(meta_data_array),  total = len(meta_data_array)):\n",
    "    if meta['doi'] != '':\n",
    "        total_doi += 1\n",
    "    \n",
    "    else:\n",
    "        exist = investigate_for_doi_manually( xml_dict['PubmedArticleSet']['PubmedArticle'][i])\n",
    "        if exist:\n",
    "            print(meta['PMID'])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8782088a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T16:00:50.587148748Z",
     "start_time": "2023-12-22T16:00:50.587005699Z"
    }
   },
   "outputs": [],
   "source": [
    "#Convert dict to query\n",
    "def convert_dict_to_query(dic):\n",
    "    if dic['year'] == '':\n",
    "        year = 0\n",
    "    \n",
    "    else:\n",
    "        year = dic['year']\n",
    "        \n",
    "    \n",
    "    if dic['pub_year'] == '':\n",
    "        pub_year = 0\n",
    "    \n",
    "    else:\n",
    "        pub_year = dic['pub_year']\n",
    "        \n",
    "        \n",
    "    query = (int(dic['PMID']), dic['journal_ISSN'], ' '.join(dic['grants']), ' '.join(dic['chemical']), \n",
    "            pub_year, year, ' '.join(dic['mesh']), dic['doi'], dic['abstract'], dic['title'] )\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812e5b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T16:00:50.587522541Z",
     "start_time": "2023-12-22T16:00:50.587365125Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3959329",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.587484530Z"
    }
   },
   "outputs": [],
   "source": [
    "query_array = []\n",
    "na = 0\n",
    "nt = 0\n",
    "for i in meta_data_array:\n",
    "    query_array.append(convert_dict_to_query(i))\n",
    "    \n",
    "#     print(i['title'])\n",
    "    if len (i['title']) > 0:\n",
    "        na +=1 \n",
    "    \n",
    "    if len (i['abstract']) > 0:\n",
    "        nt += 1\n",
    "\n",
    "print(na)\n",
    "print(nt)\n",
    "\n",
    "for query in query_array:\n",
    "    if type(query[-1]) != str:\n",
    "        print(query[-1])\n",
    "\n",
    "# insert_values_into_table(query_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da10bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T16:00:50.588556577Z",
     "start_time": "2023-12-22T16:00:50.587615105Z"
    }
   },
   "outputs": [],
   "source": [
    "def parallelize(file_path, mode='parquet', dump_address = '/shared/hossein_hm31/date_parquet/' ):\n",
    "    xml_dict = parse(file_path)\n",
    "    \n",
    "    meta_data_array = []\n",
    "\n",
    "    for i in range(len(xml_dict['PubmedArticleSet']['PubmedArticle'])):\n",
    "        rec = xml_dict['PubmedArticleSet']['PubmedArticle'][i]['MedlineCitation']\n",
    "        total = xml_dict['PubmedArticleSet']['PubmedArticle'][i]\n",
    "        x=parse_single_record(rec, total)\n",
    "        meta_data_array.append(x)\n",
    "        \n",
    "    if mode == 'postgres':\n",
    "        query_array = []\n",
    "\n",
    "        for i in meta_data_array:\n",
    "            query_array.append(convert_dict_to_query(i))\n",
    "\n",
    "\n",
    "        insert_values_into_table(query_array)\n",
    "        \n",
    "    elif mode == 'parquet':\n",
    "        try:\n",
    "            xml_name = file_path.split('/')[-1].split('.')[0] #Example '/shared/hossein_hm31/xml_data/pubmed23n0933.xml' -> pubmed23n0933.xml\n",
    "            df = pd.DataFrame(meta_data_array)\n",
    "            df.set_index(['PMID', 'doi'], inplace=True)\n",
    "            df.to_parquet(f'{dump_address+xml_name}.parquet')\n",
    "            print('saved')\n",
    "        except Exception as e: # work on python 3.x\n",
    "            print(str(e)+'\\n')\n",
    "\n",
    "\n",
    "#23 Mb vs 230 Mb single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc91576d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.587722317Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nest_dir = '/shared/hossein_hm31/xml_data/'\n",
    "files = os.listdir(nest_dir)\n",
    "\n",
    "lst = []\n",
    "\n",
    "for file in files:\n",
    "    lst.append((nest_dir+file,))\n",
    "    \n",
    "with multiprocessing.Pool(processes=80) as pool:\n",
    "    results = pool.starmap(parallelize, lst)\n",
    "    \n",
    "end = time.time()\n",
    "print(f'elapsed {end - start}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e2dd9",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "<h2> Finishing the unfinished ones </h2>\n",
    "<p> Get the incomplete files </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10db2a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.587825882Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import os\n",
    "\n",
    "obtained_files = os.listdir('/shared/hossein_hm31/date_parquet/')\n",
    "all_files = os.listdir('/shared/hossein_hm31/xml_data/')\n",
    "\n",
    "all_xml_files = [file.split('.')[0] for file in all_files if '.xml' in file]\n",
    "all_parquet_files = [file.split('.')[0] for file in obtained_files]\n",
    "\n",
    "files_left = [file for file in all_xml_files if not file in all_parquet_files]\n",
    "print(files_left)\n",
    "print(len(files_left))\n",
    "\n",
    "lst = []\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "for file in files_left:\n",
    "    lst.append((f'/shared/hossein_hm31/xml_data/{file}.xml',))\n",
    "\n",
    "\n",
    "with multiprocessing.Pool(processes=60) as pool:\n",
    "    results = pool.starmap(parallelize, lst)\n",
    "    \n",
    "end = time.time()\n",
    "print(f'elapsed {end - start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29feca",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-22T16:00:50.587925760Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
